;!function(){try { var e="undefined"!=typeof globalThis?globalThis:"undefined"!=typeof global?global:"undefined"!=typeof window?window:"undefined"!=typeof self?self:{},n=(new e.Error).stack;n&&((e._debugIds|| (e._debugIds={}))[n]="3977ce1d-d589-9929-0839-1aac398f3902")}catch(e){}}();
(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,5208113,e=>{e.v(JSON.parse('[{"name":"Qwen 3 Coder 30B A3B Instruct","chef":"alibaba","primaryModel":"qwen3-coder-30b-a3b","provider":"bedrock","type":"chat","description":"Efficient coding specialist balancing performance with cost-effectiveness for daily development tasks while maintaining strong tool integration capabilities.","contextSize":262144,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:qwen.qwen3-coder-30b-a3b-v1:0","websiteUrl":"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"Qwen 3.32B","chef":"alibaba","primaryModel":"qwen-3-32b","provider":"bedrock","type":"chat","description":"Qwen3-32B is a world-class model with comparable quality to DeepSeek R1 while outperforming GPT-4.1 and Claude Sonnet 3.7. It excels in code-gen, tool-calling, and advanced reasoning, making it an exceptional model for a wide range of production use cases.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:qwen.qwen3-32b-v1:0","websiteUrl":"https://aws.amazon.com/blogs/aws/qwen-models-are-now-available-in-amazon-bedrock/","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"Qwen 3.32B","chef":"alibaba","primaryModel":"qwen-3-32b","provider":"cerebras","type":"chat","description":"Qwen3-32B is a world-class model with comparable quality to DeepSeek R1 while outperforming GPT-4.1 and Claude Sonnet 3.7. It excels in code-gen, tool-calling, and advanced reasoning, making it an exceptional model for a wide range of production use cases.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cerebras:qwen-3-32b","websiteUrl":"https://inference-docs.cerebras.ai","modelUrl":"https://www.cerebras.ai/blog/reasoning-in-one-second-try-qwen3-32b-on-cerebras","pricingUrl":"https://inference-docs.cerebras.ai/support/pricing","inputCost":"0.4","outputCost":"0.8","isPreGateway":false},{"name":"Qwen3 235B A22B Instruct 2507","chef":"alibaba","primaryModel":"qwen-3-235b","provider":"baseten","type":"chat","description":"Mixture-of-experts LLM with math and reasoning capabilities","secondaryModels":["qwen-3-235b-a22b","qwen-3-235b-a22b-instruct-2507"],"contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/baseten:Qwen/Qwen3-235B-A22B-Instruct-2507","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/qwen3-235b-a22b-instruct-2507/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"0.22","outputCost":"0.8","isPreGateway":false},{"name":"Qwen3 235B A22b Thinking 2507","chef":"alibaba","primaryModel":"qwen-3-235b","provider":"novita","type":"chat","description":"The Qwen3-235B-A22B-Thinking-2507 represents the newest thinking-enabled model in the Qwen3 series, delivering groundbreaking improvements in reasoning capabilities. This advanced AI demonstrates significantly enhanced performance across logical reasoning, mathematics, scientific analysis, coding tasks, and academic benchmarks.","secondaryModels":["qwen-3-235b-a22b","qwen-3-235b-a22b-thinking-2507"],"contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:qwen/qwen3-235b-a22b-thinking-2507","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.3","outputCost":"3.0","isPreGateway":false},{"name":"Qwen3 Coder 480B A35B Instruct","chef":"alibaba","primaryModel":"qwen3-coder","provider":"baseten","type":"chat","description":"Mixture-of-experts LLM with advanced coding and reasoning capabilities","secondaryModels":["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"],"contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/baseten:Qwen/Qwen3-Coder-480B-A35B-Instruct","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/qwen3-coder-480b-a25b-instruct/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"1.7","outputCost":"1.7","isPreGateway":false},{"name":"Qwen3 Coder 480B A35B Instruct","chef":"alibaba","primaryModel":"qwen3-coder","provider":"deepinfra","type":"chat","description":"Qwen3-Coder-480B-A35B-Instruct is Qwen\'s most agentic code model, featuring significant performance on Agentic Coding, Agentic Browser-Use and other foundational coding tasks, achieving results comparable to Claude Sonnet.","secondaryModels":["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"],"contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:Qwen/Qwen3-Coder-480B-A35B-Instruct","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/Qwen/Qwen3-Coder-480B-A35B-Instruct","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.4","outputCost":"1.6","isPreGateway":false},{"name":"Qwen3 Coder 480B A35B Instruct","chef":"alibaba","primaryModel":"qwen3-coder","provider":"novita","type":"chat","description":"Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet’s performance in agentic programming, browser automation, and core development tasks.","secondaryModels":["qwen3-coder-480b","qwen3-coder-480b-a35b-instruct"],"contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:qwen/qwen3-coder-480b-a35b-instruct","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.64","outputCost":"2.5","isPreGateway":false},{"name":"Qwen3 Coder Plus","chef":"alibaba","primaryModel":"qwen3-coder-plus","provider":"alibaba","type":"chat","description":"Powered by Qwen3 this is a powerful Coding Agent that excels in tool calling and environment interaction to achieve autonomous programming. It combines outstanding coding proficiency with versatile general-purpose abilities.","secondaryModels":["qwen3-coder-plus-2025-09-23"],"contextSize":1000000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-coder-plus-2025-09-23","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-coder-plus","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n","inputCost":"1.0","outputCost":"5.0","isPreGateway":false},{"name":"Qwen3 Max","chef":"alibaba","primaryModel":"qwen3-max","provider":"alibaba","type":"chat","description":"The Qwen 3 series Max model has undergone specialized upgrades in agent programming and tool invocation compared to the preview version. The officially released model this time has achieved state-of-the-art (SOTA) performance in its field and is better suited to meet the demands of agents operating in more complex scenarios.","contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-max","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n","inputCost":"1.2","outputCost":"6.0","cachedInputCost":"0.24","isPreGateway":false},{"name":"Qwen3 Max Preview","chef":"alibaba","primaryModel":"qwen3-max-preview","provider":"alibaba","type":"chat","description":"Qwen3-Max-Preview shows substantial gains over the 2.5 series in overall capability, with significant enhancements in Chinese-English text understanding, complex instruction following, handling of subjective open-ended tasks, multilingual ability, and tool invocation; model knowledge hallucinations are reduced.","contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-max-preview","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n","inputCost":"1.2","outputCost":"6.0","cachedInputCost":"0.24","isPreGateway":false},{"name":"Qwen3 Next 80B A3B Instruct","chef":"alibaba","primaryModel":"qwen3-next-80b-a3b-instruct","provider":"alibaba","type":"chat","description":"A new generation of open-source, non-thinking mode model powered by Qwen3. This version demonstrates superior Chinese text understanding, augmented logical reasoning, and enhanced capabilities in text generation tasks over the previous iteration (Qwen3-235B-A22B-Instruct-2507).","contextSize":131072,"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-next-80b-a3b-instruct","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-next-80b-a3b-instruct","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models","inputCost":"0.5","outputCost":"2.0","isPreGateway":false},{"name":"Qwen3 Next 80B A3B Instruct","chef":"alibaba","primaryModel":"qwen3-next-80b-a3b-instruct","provider":"novita","type":"chat","description":"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Instruct performs comparably to our flagship model Qwen3-235B-A22B-Instruct-2507, and shows clear advantages in tasks requiring ultra-long context (up to 256K tokens).","contextSize":65536,"playgroundUrl":"https://ai-sdk.dev/playground/novita:qwen/qwen3-next-80b-a3b-instruct","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models-console/model-detail/qwen-qwen3-next-80b-a3b-instruct","pricingUrl":"https://novita.ai/pricing","inputCost":"0.15","outputCost":"1.5","isPreGateway":false},{"name":"Qwen3 Next 80B A3B Thinking","chef":"alibaba","primaryModel":"qwen3-next-80b-a3b-thinking","provider":"alibaba","type":"chat","description":"A new generation of Qwen3-based open-source thinking mode models. This version offers improved instruction following and streamlined summary responses over the previous iteration (Qwen3-235B-A22B-Thinking-2507).","contextSize":131072,"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-next-80b-a3b-thinking","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-next-80b-a3b-thinking","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models","inputCost":"0.5","outputCost":"6.0","isPreGateway":false},{"name":"Qwen3 Next 80B A3B Thinking","chef":"alibaba","primaryModel":"qwen3-next-80b-a3b-thinking","provider":"novita","type":"chat","description":"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Thinking excels at complex reasoning tasks — outperforming higher-cost models like Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking, outpeforming the closed-source Gemini-2.5-Flash-Thinking on multiple benchmarks, and approaching the performance of our top-tier model Qwen3-235B-A22B-Thinking-2507.","contextSize":65536,"playgroundUrl":"https://ai-sdk.dev/playground/novita:qwen/qwen3-next-80b-a3b-thinking","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models-console/model-detail/qwen-qwen3-next-80b-a3b-thinking","pricingUrl":"https://novita.ai/pricing","inputCost":"0.15","outputCost":"1.5","isPreGateway":false},{"name":"Qwen3 VL 235B A22B Instruct","chef":"alibaba","primaryModel":"qwen3-vl-instruct","provider":"alibaba","type":"chat","description":"The Qwen3 series VL models has been comprehensively upgraded in areas such as visual coding and spatial perception. Its visual perception and recognition capabilities have significantly improved, supporting the understanding of ultra-long videos, and its OCR functionality has undergone a major enhancement.","secondaryModels":["qwen3-vl-235b-a22b-instruct"],"contextSize":131072,"tags":["vision"],"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-vl-235b-a22b-instruct","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-vl-235b-a22b-instruct","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n","inputCost":"0.7","outputCost":"2.8","isPreGateway":false},{"name":"Qwen3 VL 235B A22B Thinking","chef":"alibaba","primaryModel":"qwen3-vl-thinking","provider":"alibaba","type":"chat","description":"Qwen3 series VL models feature significantly enhanced multimodal reasoning capabilities, with a particular focus on optimizing the model for STEM and mathematical reasoning. Visual perception and recognition abilities have been comprehensively improved, and OCR capabilities have undergone a major upgrade.","secondaryModels":["qwen3-vl-235b-a22b-thinking"],"contextSize":131072,"tags":["vision"],"playgroundUrl":"https://ai-sdk.dev/playground/alibaba:qwen3-vl-235b-a22b-thinking","websiteUrl":"https://www.alibabacloud.com/product/model-studio","modelUrl":"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-vl-235b-a22b-thinking","pricingUrl":"https://www.alibabacloud.com/help/en/model-studio/models#161e7d77cfv1n","inputCost":"0.7","outputCost":"8.4","isPreGateway":false},{"name":"Qwen3-14B","chef":"alibaba","primaryModel":"qwen-3-14b","provider":"deepinfra","type":"chat","description":"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support","secondaryModels":["alias-1","alias-2"],"contextSize":40960,"tags":["reasoning","tool-use","tag-1"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:Qwen/Qwen3-14B","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/Qwen/Qwen3-14B","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.06","outputCost":"0.24","isPreGateway":false},{"name":"Qwen3-235B-A22B","chef":"alibaba","primaryModel":"qwen-3-235b","provider":"deepinfra","type":"chat","description":"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support","secondaryModels":["qwen-3-235b-a22b"],"contextSize":40960,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:Qwen/Qwen3-235B-A22B","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/Qwen/Qwen3-235B-A22B","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.13","outputCost":"0.6","isPreGateway":false},{"name":"Qwen3-235B-A22B","chef":"alibaba","primaryModel":"qwen-3-235b","provider":"fireworks","type":"chat","description":"Qwen3 235B with 22B active parameter model.","secondaryModels":["qwen3-235b-a22b"],"contextSize":32768,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/qwen3-235b-a22b","websiteUrl":"https://fireworks.ai","modelUrl":"https://fireworks.ai/models/fireworks/qwen3-235b-a22b","pricingUrl":"https://fireworks.ai/pricing","inputCost":"0.9","outputCost":"0.9","isPreGateway":false},{"name":"Qwen3-30B-A3B","chef":"alibaba","primaryModel":"qwen-3-30b","provider":"deepinfra","type":"chat","description":"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support","secondaryModels":["qwen-3-30b-a3b"],"contextSize":40960,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:Qwen/Qwen3-30B-A3B","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/Qwen/Qwen3-235B-A22B","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.08","outputCost":"0.29","isPreGateway":false},{"name":"Qwen3-32B","chef":"alibaba","primaryModel":"qwen-3-32b","provider":"deepinfra","type":"chat","description":"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support","contextSize":40960,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:Qwen/Qwen3-32B","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/Qwen/Qwen3-32B","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.1","outputCost":"0.3","isPreGateway":false},{"name":"Qwen3-32B","chef":"alibaba","primaryModel":"qwen-3-32b","provider":"groq","type":"chat","description":"Qwen 3 32B is the latest generation of large language models in the Qwen series, offering groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support. It uniquely supports seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within a single model. The model excels in human preference alignment, creative writing, role-playing, and multi-turn dialogues, while supporting 100+ languages and dialects.","secondaryModels":["qwen3-32b"],"contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:qwen/qwen3-32b","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/model/qwen/qwen3-32b","pricingUrl":"https://www.groq.com/","inputCost":"0.29","outputCost":"0.59","isPreGateway":false},{"name":"Nova Lite","chef":"amazon","primaryModel":"nova-lite","provider":"bedrock","type":"chat","description":"A very low cost multimodal model that is lightning fast for processing image, video, and text inputs.","secondaryModels":["nova-lite-v1"],"contextSize":300000,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.amazon.nova-lite-v1:0","websiteUrl":"https://aws.amazon.com/ai/generative-ai/nova","modelUrl":"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.06","outputCost":"0.24","isPreGateway":false},{"name":"Nova Micro","chef":"amazon","primaryModel":"nova-micro","provider":"bedrock","type":"chat","description":"A text-only model that delivers the lowest latency responses at very low cost.","secondaryModels":["nova-micro-v1"],"contextSize":128000,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.amazon.nova-micro-v1:0","websiteUrl":"https://aws.amazon.com/ai/generative-ai/nova","modelUrl":"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.035","outputCost":"0.14","isPreGateway":false},{"name":"Nova Pro","chef":"amazon","primaryModel":"nova-pro","provider":"bedrock","type":"chat","description":"A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks.","secondaryModels":["nova-pro-v1"],"contextSize":300000,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.amazon.nova-pro-v1:0","websiteUrl":"https://aws.amazon.com/ai/generative-ai/nova","modelUrl":"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.8","outputCost":"3.2","isPreGateway":false},{"name":"Titan Text Embeddings V2","chef":"amazon","primaryModel":"titan-embed-text-v2","provider":"bedrock","type":"embedding","description":"Amazon Titan Text Embeddings V2 is a light weight, efficient multilingual embedding model supporting 1024, 512, and 256 dimensions.","secondaryModels":["amazon.titan-embed-text-v2"],"contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:amazon.titan-embed-text-v2:0","websiteUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.02","outputCost":"0","isPreGateway":false},{"name":"Claude 3 Haiku","chef":"anthropic","primaryModel":"claude-3-haiku","provider":"anthropic","type":"chat","description":"Claude 3 Haiku is Anthropic\'s fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku to quickly analyze large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.","secondaryModels":["claude-3-haiku-20240307","claude-v3-haiku"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-3-haiku-20240307","websiteUrl":"https://www.anthropic.com","modelUrl":"https://docs.anthropic.com/claude/docs/models-overview","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"0.25","outputCost":"1.25","cachedInputCost":"0.03","cacheCreationInputCost":"0.3","isPreGateway":false},{"name":"Claude 3 Haiku","chef":"anthropic","primaryModel":"claude-3-haiku","provider":"bedrock","type":"chat","description":"Claude 3 Haiku is Anthropic\'s fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.","secondaryModels":["claude-3-haiku-20240307-v1","anthropic.claude-3-haiku-20240307-v1:0"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-haiku-20240307-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.25","outputCost":"1.25","isPreGateway":false},{"name":"Claude 3 Haiku","chef":"anthropic","primaryModel":"claude-3-haiku","provider":"vertexAnthropic","type":"chat","description":"Claude 3 Haiku is Anthropic\'s fastest vision and text model for near-instant responses to simple queries, meant for seamless AI experiences mimicking human interactions.","secondaryModels":["claude-3-haiku@20240307"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-haiku@20240307","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-haiku","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"0.25","outputCost":"1.25","cachedInputCost":"0.03","cacheCreationInputCost":"0.3","isPreGateway":false},{"name":"Claude 3 Opus","chef":"anthropic","primaryModel":"claude-3-opus","provider":"anthropic","type":"chat","description":"Claude 3 Opus is Anthropic\'s most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what\'s possible with generative AI.","secondaryModels":["claude-3-opus-20240229","claude-v3-opus","claude-3-opus-latest"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-3-opus-20240229","websiteUrl":"https://www.anthropic.com","modelUrl":"https://docs.anthropic.com/claude/docs/models-overview","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","isPreGateway":false},{"name":"Claude 3 Opus","chef":"anthropic","primaryModel":"claude-3-opus","provider":"bedrock","type":"chat","description":"Claude 3 Opus is Anthropic\'s most powerful AI model, with state-of-the-art performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Claude 3 Opus shows us the frontier of what\'s possible with generative AI. Claude 3 Opus can process images and return text outputs, and features a 200K context window.","secondaryModels":["claude-3-opus-20240229-v1","anthropic.claude-3-opus-20240229-v1:0"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-opus-20240229-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"15.0","outputCost":"75.0","isPreGateway":false},{"name":"Claude 3 Opus","chef":"anthropic","primaryModel":"claude-3-opus","provider":"vertexAnthropic","type":"chat","description":"Claude 3 Opus is a powerful AI model, with top-level performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding.","secondaryModels":["claude-3-opus@20240229"],"contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-opus@20240229","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-opus","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","isPreGateway":false},{"name":"Claude 3.5 Haiku","chef":"anthropic","primaryModel":"claude-3.5-haiku","provider":"anthropic","type":"chat","description":"Claude 3.5 Haiku is the next generation of our fastest model. For a similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks.","secondaryModels":["claude-3-5-haiku-20241022","claude-v3.5-haiku","claude-3-5-haiku-latest"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-3-5-haiku-20241022","websiteUrl":"https://www.anthropic.com","modelUrl":"https://www.anthropic.com/claude/haiku","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"0.8","outputCost":"4.0","cachedInputCost":"0.08","cacheCreationInputCost":"1.0","isPreGateway":false},{"name":"Claude 3.5 Haiku","chef":"anthropic","primaryModel":"claude-3.5-haiku","provider":"bedrock","type":"chat","description":"Claude 3 Haiku is Anthropic\'s fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with speed. Customers will be able to build seamless AI experiences that mimic human interactions. Claude 3 Haiku can process images and return text outputs, and features a 200K context window.","secondaryModels":["claude-3-5-haiku-20241022-v1","anthropic.claude-3-5-haiku-20241022-v1:0","claude-3-5-haiku-20241022"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/about-aws/whats-new/2024/11/anthropics-claude-3-5-haiku-model-amazon-bedrock/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.8","outputCost":"4.0","cachedInputCost":"0.08","cacheCreationInputCost":"1.0","isPreGateway":false},{"name":"Claude 3.5 Haiku","chef":"anthropic","primaryModel":"claude-3.5-haiku","provider":"vertexAnthropic","type":"chat","description":"Claude 3.5 Haiku, Anthropic\'s fastest and most cost-effective model, excels at use cases like code and test case generation, sub-agents, and user-facing chatbots.","secondaryModels":["claude-3-5-haiku@20241022","claude-3-5-haiku-20241022"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-5-haiku@20241022","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-haiku","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"0.8","outputCost":"4.0","cachedInputCost":"0.08","cacheCreationInputCost":"1.0","isPreGateway":false},{"name":"Claude 3.5 Sonnet","chef":"anthropic","primaryModel":"claude-3.5-sonnet","provider":"bedrock","type":"chat","description":"The upgraded Claude 3.5 Sonnet is now state-of-the-art for a variety of tasks including real-world software engineering, agentic capabilities and computer use. The new Claude 3.5 Sonnet delivers these advancements at the same price and speed as its predecessor.","secondaryModels":["claude-3-5-sonnet-20241022","claude-3-5-sonnet-20241022-v2","anthropic.claude-3-5-sonnet-20241022-v2:0"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude 3.5 Sonnet","chef":"anthropic","primaryModel":"claude-3.5-sonnet","provider":"vertexAnthropic","type":"chat","description":"The upgraded Claude 3.5 Sonnet is now state-of-the-art for a variety of tasks including real-world software engineering, enhanced agentic capabilities, and computer use.","secondaryModels":["claude-3-5-sonnet","claude-3-5-sonnet-20241022","claude-3-5-sonnet-v2@20241022"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-5-sonnet-v2@20241022","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-v2-sonnet","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude 3.5 Sonnet (2024-06-20)","chef":"anthropic","primaryModel":"claude-3.5-sonnet-20240620","provider":"bedrock","type":"chat","description":"Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.","secondaryModels":["claude-3-5-sonnet-20240620","claude-3-5-sonnet-20240620-v1","anthropic.claude-3-5-sonnet-20240620-v1:0"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-5-sonnet-20240620-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"3.0","outputCost":"15.0","isPreGateway":false},{"name":"Claude 3.5 Sonnet (2024-06-20)","chef":"anthropic","primaryModel":"claude-3.5-sonnet-20240620","provider":"vertexAnthropic","type":"chat","description":"Claude 3.5 Sonnet outperforms Claude 3 Opus on a wide range of Anthropic\'s evaluations with the speed and cost of Anthropic\'s mid-tier model, Claude 3 Sonnet.","secondaryModels":["claude-3-5-sonnet-20240620","claude-3-5-sonnet@20240620"],"contextSize":200000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-5-sonnet@20240620","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-5-sonnet","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude 3.7 Sonnet","chef":"anthropic","primaryModel":"claude-3.7-sonnet","provider":"anthropic","type":"chat","description":"Claude 3.7 Sonnet is the first hybrid reasoning model and Anthropic\'s most intelligent model to date. It delivers state-of-the-art performance for coding, content generation, data analysis, and planning tasks, building upon its predecessor Claude 3.5 Sonnet\'s capabilities in software engineering and computer use.","secondaryModels":["claude-3-7-sonnet-20250219","claude-3.7-sonnet-reasoning","claude-3-7-sonnet-latest","claude-3-7-sonnet"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-3-7-sonnet-20250219","websiteUrl":"https://www.anthropic.com","modelUrl":"https://www.anthropic.com/claude/sonnet","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude 3.7 Sonnet","chef":"anthropic","primaryModel":"claude-3.7-sonnet","provider":"bedrock","type":"chat","description":"Claude 3.7 Sonnet is Anthropic\'s most intelligent model to date and the first Claude model to offer extended thinking—the ability to solve complex problems with careful, step-by-step reasoning. Anthropic is the first AI lab to introduce a single model where users can balance speed and quality by choosing between standard thinking for near-instant responses or extended thinking or advanced reasoning. Claude 3.7 Sonnet is state-of-the-art for coding, and delivers advancements in computer use, agentic capabilities, complex reasoning, and content generation. With frontier performance and more control over speed, Claude 3.7 Sonnet is the ideal choice for powering AI agents, especially customer-facing agents, and complex AI workflows.","secondaryModels":["claude-3-7-sonnet-20250219-v1","anthropic.claude-3-7-sonnet-20250219-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-37.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude 3.7 Sonnet","chef":"anthropic","primaryModel":"claude-3.7-sonnet","provider":"vertexAnthropic","type":"chat","description":"A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks.","secondaryModels":["claude-3-7-sonnet@20250219"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-3-7-sonnet@20250219","websiteUrl":"https://cloud.google.com/blog/products/ai-machine-learning/anthropics-claude-3-7-sonnet-is-available-on-vertex-ai","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-7-sonnet","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude Haiku 4.5","chef":"anthropic","primaryModel":"claude-haiku-4.5","provider":"anthropic","type":"chat","description":"Claude Haiku 4.5 matches Sonnet 4\'s performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.","secondaryModels":["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001","claude-haiku-4-5-20251001","anthropic.claude-haiku-4-5-20251001-v1:0","claude-haiku-4-5@20251001","claude-haiku-4-5"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-haiku-4-5-20251001","websiteUrl":"https://www.anthropic.com","modelUrl":"https://www.anthropic.com/claude/sonnet","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"1","outputCost":"5","cachedInputCost":"0.10","cacheCreationInputCost":"1.25","webSearchCallCost":"10.0","isPreGateway":false},{"name":"Claude Haiku 4.5","chef":"anthropic","primaryModel":"claude-haiku-4.5","provider":"bedrock","type":"chat","description":"Claude Haiku 4.5 matches Sonnet 4\'s performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.","secondaryModels":["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001","claude-haiku-4-5-20251001","claude-haiku-4-5","claude-haiku-4-5@20251001","anthropic.claude-haiku-4-5-20251001-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:global.anthropic.claude-haiku-4-5-20251001-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"1","outputCost":"5","cachedInputCost":"0.10","cacheCreationInputCost":"1.25","isPreGateway":false},{"name":"Claude Haiku 4.5","chef":"anthropic","primaryModel":"claude-haiku-4.5","provider":"vertexAnthropic","type":"chat","description":"Claude Haiku 4.5 matches Sonnet 4\'s performance on coding, computer use, and agent tasks at substantially lower cost and faster speeds. It delivers near-frontier performance and Claude’s unique character at a price point that works for scaled sub-agent deployments, free tier products, and intelligence-sensitive applications with budget constraints.","secondaryModels":["claude-4.5-haiku","claude-4.5-haiku-20251001","claude-haiku-4.5-20251001","claude-4.5-haiku-20251001-v1","claude-haiku-4.5-20251001-v1","anthropic.claude-haiku-4.5-20251001-v1:0","claude-haiku-4.5@20251001","claude-haiku-4-5-20251001","claude-haiku-4-5","claude-haiku-4-5@20251001","anthropic.claude-haiku-4-5-20251001-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-haiku-4-5@20251001","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-haiku-4.5","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"1","outputCost":"5","cachedInputCost":"0.10","cacheCreationInputCost":"1.25","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Claude Opus 4","chef":"anthropic","primaryModel":"claude-opus-4","provider":"anthropic","type":"chat","description":"Claude Opus 4 is Anthropic\'s most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.","secondaryModels":["claude-4-opus-20250514","claude-4-opus","claude-opus-4-20250514"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-opus-4-20250514","websiteUrl":"https://www.anthropic.com","modelUrl":"https://docs.anthropic.com/claude/docs/models-overview","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","webSearchCallCost":"10.0","isPreGateway":false},{"name":"Claude Opus 4","chef":"anthropic","primaryModel":"claude-opus-4","provider":"bedrock","type":"chat","description":"Claude Opus 4 is Anthropic\'s most intelligent model and is state-of-the-art for coding and agent capabilities, especially agentic search. It excels for customers needing frontier intelligence: Advanced coding: Independently plan and execute complex development tasks end-to-end. It adapts to your style and maintains high code quality throughout.","secondaryModels":["claude-4-opus-20250514-v1","claude-opus-4-20250514-v1","claude-4-opus","anthropic.claude-opus-4-20250514-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-opus-4-20250514-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","isPreGateway":false},{"name":"Claude Opus 4","chef":"anthropic","primaryModel":"claude-opus-4","provider":"vertexAnthropic","type":"chat","description":"Claude Opus 4 is Anthropic\'s most powerful model yet and the state-of-the-art coding model. It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, significantly expanding what AI agents can solve. Claude Opus 4 is ideal for powering frontier agent products and features.","secondaryModels":["claude-opus-4@20250514","claude-4-opus"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-opus-4@20250514","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-opus-4","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Claude Opus 4.1","chef":"anthropic","primaryModel":"claude-opus-4.1","provider":"anthropic","type":"chat","description":"Claude Opus 4.1 is a drop-in replacement for Opus 4 that delivers superior performance and precision for real-world coding and agentic tasks. Opus 4.1 advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, and handles complex, multi-step problems with more rigor and attention to detail.","secondaryModels":["claude-opus-4-1-20250805","claude-4-1-opus-20250805","claude-4.1-opus","claude-opus-4-1@20250805","anthropic.claude-opus-4-1-20250805-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-opus-4-1-20250805","websiteUrl":"https://www.anthropic.com","modelUrl":"https://docs.anthropic.com/claude/docs/models-overview","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","webSearchCallCost":"10.0","isPreGateway":false},{"name":"Claude Opus 4.1","chef":"anthropic","primaryModel":"claude-opus-4.1","provider":"bedrock","type":"chat","description":"Claude Opus 4.1 is a drop-in replacement for Opus 4 that delivers superior performance and precision for real-world coding and agentic tasks. Opus 4.1 advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, and handles complex, multi-step problems with more rigor and attention to detail.","secondaryModels":["claude-opus-4-1-20250805","claude-4-1-opus-20250805","claude-4.1-opus","claude-opus-4-1@20250805","anthropic.claude-opus-4-1-20250805-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-opus-4-1-20250805-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/anthropic/","modelUrl":"https://aws.amazon.com/bedrock/anthropic/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","isPreGateway":false},{"name":"Claude Opus 4.1","chef":"anthropic","primaryModel":"claude-opus-4.1","provider":"vertexAnthropic","type":"chat","description":"Claude Opus 4.1 is a drop-in replacement for Opus 4 that delivers superior performance and precision for real-world coding and agentic tasks. Opus 4.1 advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, and handles complex, multi-step problems with more rigor and attention to detail.","secondaryModels":["claude-opus-4-1-20250805","claude-4-1-opus-20250805","claude-4.1-opus","claude-opus-4-1@20250805","anthropic.claude-opus-4-1-20250805-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-opus-4-1@20250805","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude/opus-4-1","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"15.0","outputCost":"75.0","cachedInputCost":"1.5","cacheCreationInputCost":"18.75","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Claude Sonnet 4","chef":"anthropic","primaryModel":"claude-sonnet-4","provider":"anthropic","type":"chat","description":"Claude Sonnet 4 significantly improves on Sonnet 3.7\'s industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.","secondaryModels":["claude-4-sonnet","claude-4-sonnet-20250514","claude-sonnet-4-20250514","claude-4-sonnet-20250514-v1","claude-sonnet-4-20250514-v1","anthropic.claude-sonnet-4-20250514-v1:0","claude-sonnet-4@20250514"],"providerModelVariants":[{"slug":"context-1m-2025-08-07","description":"Claude Sonnet 4 with 1M context window","detailedDescription":"Claude Sonnet 4 now supports a 1M-token context (beta); opt in by sending the `anthropic-beta: context-1m-2025-08-07` header. See the announcement and docs: [news](https://www.anthropic.com/news/1m-context), [context windows](https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window).\\n\\nPricing: only if total input tokens (prompt + cache reads/writes) exceed 200K, input is charged 2× and output 1.5×; otherwise standard rates apply. Details: [long context pricing](https://docs.anthropic.com/en/docs/about-claude/pricing#long-context-pricing)."}],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-sonnet-4-20250514","websiteUrl":"https://www.anthropic.com","modelUrl":"https://www.anthropic.com/claude/sonnet","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","webSearchCallCost":"10.0","isPreGateway":false},{"name":"Claude Sonnet 4","chef":"anthropic","primaryModel":"claude-sonnet-4","provider":"bedrock","type":"chat","description":"Claude Sonnet 4 balances impressive performance for coding with the right speed and cost for high-volume use cases: Coding: Handle everyday development tasks with enhanced performance-power code reviews, bug fixes, API integrations, and feature development with immediate feedback loops.","secondaryModels":["claude-4-sonnet","claude-4-sonnet-20250514","claude-sonnet-4-20250514","claude-4-sonnet-20250514-v1","claude-sonnet-4-20250514-v1","anthropic.claude-sonnet-4-20250514-v1:0","claude-sonnet-4@20250514"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.anthropic.claude-sonnet-4-20250514-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","isPreGateway":false},{"name":"Claude Sonnet 4","chef":"anthropic","primaryModel":"claude-sonnet-4","provider":"vertexAnthropic","type":"chat","description":"Claude Sonnet 4 is Anthropic\'s mid-size model with several major improvements - especially for coding. It is the ideal balance of performance and practicality for most internal and external use cases, including user-facing AI agents. The model is also more steerable than before, providing enhanced control over its eagerness to implement changes.","secondaryModels":["claude-4-sonnet","claude-4-sonnet-20250514","claude-sonnet-4-20250514","claude-4-sonnet-20250514-v1","claude-sonnet-4-20250514-v1","anthropic.claude-sonnet-4-20250514-v1:0","claude-sonnet-4@20250514"],"providerModelVariants":[{"slug":"context-1m-2025-08-07","description":"Claude Sonnet 4 with 1M context window","detailedDescription":"Claude Sonnet 4 now supports a 1M-token context (beta); opt in by sending the `anthropic-beta: context-1m-2025-08-07` header. See the announcement and docs: [news](https://www.anthropic.com/news/1m-context), [context windows](https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window).\\n\\nPricing: only if total input tokens (prompt + cache reads/writes) exceed 200K, input is charged 2× and output 1.5×; otherwise standard rates apply. Details: [long context pricing](https://docs.anthropic.com/en/docs/about-claude/pricing#long-context-pricing)."}],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-sonnet-4@20250514","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-sonnet-4","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Claude Sonnet 4.5","chef":"anthropic","primaryModel":"claude-sonnet-4.5","provider":"anthropic","type":"chat","description":"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.","secondaryModels":["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929","claude-sonnet-4-5-20250929","claude-sonnet-4-5","claude-sonnet-4-5@20250929","anthropic.claude-sonnet-4-5-20250929-v1:0"],"providerModelVariants":[{"slug":"context-1m-2025-08-07","description":"Claude Sonnet 4.5 with 1M context window","detailedDescription":"Claude Sonnet 4.5 now supports a 1M-token context (beta); opt in by sending the `anthropic-beta: context-1m-2025-08-07` header. See the announcement and docs: [news](https://www.anthropic.com/news/1m-context), [context windows](https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window).\\n\\nPricing: only if total input tokens (prompt + cache reads/writes) exceed 200K, input is charged 2× and output 1.5×; otherwise standard rates apply. Details: [long context pricing](https://docs.anthropic.com/en/docs/about-claude/pricing#long-context-pricing)."}],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/anthropic:claude-sonnet-4-5-20250929","websiteUrl":"https://www.anthropic.com","modelUrl":"https://www.anthropic.com/claude/sonnet","pricingUrl":"https://www.anthropic.com/pricing#anthropic-api","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","webSearchCallCost":"10.0","isPreGateway":false},{"name":"Claude Sonnet 4.5","chef":"anthropic","primaryModel":"claude-sonnet-4.5","provider":"bedrock","type":"chat","description":"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.","secondaryModels":["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929","claude-sonnet-4-5-20250929","claude-sonnet-4-5","claude-sonnet-4-5@20250929","anthropic.claude-sonnet-4-5-20250929-v1:0"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:global.anthropic.claude-sonnet-4-5-20250929-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/claude/","modelUrl":"https://aws.amazon.com/bedrock/claude/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"3.3","outputCost":"16.5","cachedInputCost":"0.33","cacheCreationInputCost":"4.125","isPreGateway":false},{"name":"Claude Sonnet 4.5","chef":"anthropic","primaryModel":"claude-sonnet-4.5","provider":"vertexAnthropic","type":"chat","description":"Claude Sonnet 4.5 is the newest model in the Sonnet series, offering improvements and updates over Sonnet 4.","secondaryModels":["claude-4.5-sonnet","claude-4.5-sonnet-20250929","claude-sonnet-4.5-20250929","claude-4.5-sonnet-20250929-v1","claude-sonnet-4.5-20250929-v1","anthropic.claude-sonnet-4.5-20250929-v1:0","claude-sonnet-4.5@20250929","claude-sonnet-4-5-20250929","claude-sonnet-4-5","claude-sonnet-4-5@20250929","anthropic.claude-sonnet-4-5-20250929-v1:0"],"providerModelVariants":[{"slug":"context-1m-2025-08-07","description":"Claude Sonnet 4.5 with 1M context window","detailedDescription":"Claude Sonnet 4.5 now supports a 1M-token context (beta); opt in by sending the `anthropic-beta: context-1m-2025-08-07` header. See the announcement and docs: [news](https://www.anthropic.com/news/1m-context), [context windows](https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window).\\n\\nPricing: only if total input tokens (prompt + cache reads/writes) exceed 200K, input is charged 2× and output 1.5×; otherwise standard rates apply. Details: [long context pricing](https://docs.anthropic.com/en/docs/about-claude/pricing#long-context-pricing)."}],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertexAnthropic:claude-sonnet-4-5@20250929","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-sonnet-4.5","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing#claude-models","inputCost":"3.0","outputCost":"15.0","cachedInputCost":"0.3","cacheCreationInputCost":"3.75","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Command A","chef":"cohere","primaryModel":"command-a","provider":"cohere","type":"chat","description":"Command A is Cohere\'s most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024.","secondaryModels":["command-a-03-2025"],"contextSize":256000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cohere:command-a-03-2025","websiteUrl":"https://cohere.com","modelUrl":"https://docs.cohere.com/v2/docs/command-a","pricingUrl":"https://cohere.com/pricing","inputCost":"2.5","outputCost":"10.0","isPreGateway":false},{"name":"Embed v4.0","chef":"cohere","primaryModel":"embed-v4.0","provider":"cohere","type":"embedding","description":"A model that allows for text, images, or mixed content to be classified or turned into embeddings.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/cohere:embed-v4.0","websiteUrl":"https://cohere.com","modelUrl":"https://docs.cohere.com/docs/cohere-embed","pricingUrl":"https://cohere.com/pricing","inputCost":"0.12","outputCost":"0","isPreGateway":false},{"name":"DeepSeek R1 0528","chef":"deepseek","primaryModel":"deepseek-r1","provider":"parasail","type":"chat","description":"The latest revision of DeepSeek\'s first-generation reasoning model","secondaryModels":["deepseek-r1-0528"],"contextSize":163840,"tags":["reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/parasail:parasail-deepseek-r1-0528","websiteUrl":"https://www.parasail.io/","modelUrl":"https://www.parasail.io/","pricingUrl":"https://www.saas.parasail.io/pricing","inputCost":"0.79","outputCost":"4.00","isPreGateway":false},{"name":"DeepSeek V3 0324","chef":"deepseek","primaryModel":"deepseek-v3","provider":"baseten","type":"chat","description":"Fast general-purpose LLM with enhanced reasoning capabilities","secondaryModels":["deepseek-v3-0324"],"contextSize":163840,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/baseten:deepseek-ai/DeepSeek-V3-0324","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/deepseek-v3/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"0.77","outputCost":"0.77","isPreGateway":false},{"name":"DeepSeek V3 0324","chef":"deepseek","primaryModel":"deepseek-v3","provider":"novita","type":"chat","description":"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.","secondaryModels":["deepseek-v3-0324"],"contextSize":163840,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:deepseek/deepseek-v3-0324","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.28","outputCost":"1.14","isPreGateway":false},{"name":"DeepSeek V3.1","chef":"deepseek","primaryModel":"deepseek-v3.1","provider":"deepinfra","type":"chat","description":"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens. Additionally, DeepSeek-V3.1 is trained using the UE8M0 FP8 scale data format to ensure compatibility with microscaling data formats.","contextSize":163840,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:deepseek-ai/DeepSeek-V3.1","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/deepseek-ai/DeepSeek-V3.1","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.3","outputCost":"1.0","isPreGateway":false},{"name":"DeepSeek V3.1","chef":"deepseek","primaryModel":"deepseek-v3.1","provider":"novita","type":"chat","description":"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.","contextSize":163840,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:deepseek/deepseek-v3-0324","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.55","outputCost":"1.66","isPreGateway":false},{"name":"DeepSeek V3.1 Terminus","chef":"deepseek","primaryModel":"deepseek-v3.1-terminus","provider":"novita","type":"chat","description":"DeepSeek-V3.1-Terminus delivers more stable & reliable outputs across benchmarks compared to the previous version and addresses user feedback (i.e. language consistency and agent upgrades).","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:deepseek/deepseek-v3.1-terminus","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models/model-detail/deepseek-deepseek-v3.1-terminus","pricingUrl":"https://novita.ai/pricing","inputCost":"0.27","outputCost":"1.0","isPreGateway":false},{"name":"DeepSeek V3.2 Exp","chef":"deepseek","primaryModel":"deepseek-v3.2-exp","provider":"deepseek","type":"chat","description":"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.","secondaryModels":["deepseek-chat"],"contextSize":163840,"tags":["implicit-caching","reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepseek:deepseek-chat","websiteUrl":"https://www.deepseek.com","modelUrl":"https://api-docs.deepseek.com/news/news250929","pricingUrl":"https://api-docs.deepseek.com/quick_start/pricing","inputCost":"0.28","outputCost":"0.42","cachedInputCost":"0.028","isPreGateway":false},{"name":"DeepSeek V3.2 Exp","chef":"deepseek","primaryModel":"deepseek-v3.2-exp","provider":"novita","type":"chat","description":"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.","contextSize":163840,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:deepseek/deepseek-v3.2-exp","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models/model-detail/deepseek-deepseek-v3.2-exp","pricingUrl":"https://novita.ai/pricing","inputCost":"0.27","outputCost":"0.41","isPreGateway":false},{"name":"DeepSeek V3.2 Exp Thinking","chef":"deepseek","primaryModel":"deepseek-v3.2-exp-thinking","provider":"deepseek","type":"chat","description":"DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency. Built on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality.","secondaryModels":["deepseek-reasoner"],"contextSize":163840,"tags":["implicit-caching","reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepseek:deepseek-reasoner","websiteUrl":"https://www.deepseek.com","modelUrl":"https://api-docs.deepseek.com/news/news250929","pricingUrl":"https://api-docs.deepseek.com/quick_start/pricing","inputCost":"0.28","outputCost":"0.42","cachedInputCost":"0.028","isPreGateway":false},{"name":"DeepSeek-R1","chef":"deepseek","primaryModel":"deepseek-r1","provider":"bedrock","type":"chat","description":"DeepSeek-R1 provides customers a state-of-the-art reasoning model, optimized for general reasoning tasks, math, science, and code generation.","secondaryModels":["deepseek.r1-v1"],"contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.deepseek.r1-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/deepseek/","modelUrl":"https://aws.amazon.com/bedrock/deepseek/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"1.35","outputCost":"5.4","isPreGateway":false},{"name":"DeepSeek-V3.1","chef":"deepseek","primaryModel":"deepseek-v3.1","provider":"baseten","type":"chat","description":"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.","contextSize":163840,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/baseten:deepseek-ai/DeepSeek-V3.1","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/deepseek-v3-1/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"0.50","outputCost":"1.50","isPreGateway":false},{"name":"DeepSeek-V3.1","chef":"deepseek","primaryModel":"deepseek-v3.1","provider":"fireworks","type":"chat","description":"DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. DeepSeek has expanded their dataset by collecting additional long documents and substantially extending both training phases.","contextSize":163840,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/deepseek-v3p1","websiteUrl":"https://www.fireworks.ai/","modelUrl":"https://fireworks.ai/models/fireworks/deepseek-v3p1","pricingUrl":"https://fireworks.ai/pricing#text","inputCost":"0.56","outputCost":"1.68","isPreGateway":false},{"name":"Gemini 2.0 Flash","chef":"google","primaryModel":"gemini-2.0-flash","provider":"vertex","type":"chat","description":"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.","secondaryModels":["gemini-2.0-flash-001"],"contextSize":1048576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-001","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.15","outputCost":"0.6","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.0 Flash","chef":"google","primaryModel":"gemini-2.0-flash","provider":"google","type":"chat","description":"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window.","secondaryModels":["gemini-2.0-flash-001"],"contextSize":1000000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.0-flash-001","websiteUrl":"https://developers.googleblog.com/en/gemini-2-family-expands","modelUrl":"https://ai.google.dev/gemini-api/docs/models/gemini-v2","pricingUrl":"https://ai.google.dev/pricing","inputCost":"0.1","outputCost":"0.4","cachedInputCost":"0.025","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.0 Flash Lite","chef":"google","primaryModel":"gemini-2.0-flash-lite","provider":"vertex","type":"chat","description":"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.","secondaryModels":["gemini-2.0-flash-lite-001"],"contextSize":1048576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.0-flash-lite-001","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.075","outputCost":"0.3","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.0 Flash Lite","chef":"google","primaryModel":"gemini-2.0-flash-lite","provider":"google","type":"chat","description":"Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.","secondaryModels":["gemini-2.0-flash-lite-001"],"contextSize":1048576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.0-flash-lite-001","websiteUrl":"https://developers.googleblog.com/en/gemini-2-family-expands","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash-lite","pricingUrl":"https://ai.google.dev/pricing","inputCost":"0.075","outputCost":"0.3","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash","chef":"google","primaryModel":"gemini-2.5-flash","provider":"vertex","type":"chat","description":"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.","contextSize":1000000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.5-flash","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.3","outputCost":"2.5","cachedInputCost":"0.03","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash","chef":"google","primaryModel":"gemini-2.5-flash","provider":"google","type":"chat","description":"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.","contextSize":1000000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash","websiteUrl":"https://developers.googleblog.com/en/start-building-with-gemini-25-flash","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash","pricingUrl":"https://ai.google.dev/pricing","inputCost":"0.3","outputCost":"2.5","cachedInputCost":"0.03","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash Image (Code name: Nano Banana)","chef":"google","primaryModel":"gemini-2.5-flash-image","provider":"google","type":"chat","description":"Gemini 2.5 Flash Image is our first fully hybrid reasoning model, letting developers turn thinking on or off and set thinking budgets to balance quality, cost, and latency. Upgraded for rapid creative workflows, it can generate interleaved text and images and supports conversational, multi‑turn image editing in natural language. It’s also locale‑aware, enabling culturally and linguistically appropriate image generation for audiences worldwide.","secondaryModels":[],"contextSize":32768,"tags":["image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash-image","websiteUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-image","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-image","pricingUrl":"https://ai.google.dev/pricing","inputCost":"0.3","outputCost":"2.5","isPreGateway":false},{"name":"Gemini 2.5 Flash Image (Code name: Nano Banana)","chef":"google","primaryModel":"gemini-2.5-flash-image","provider":"vertex","type":"chat","description":"Gemini 2.5 Flash Image is our first fully hybrid reasoning model, letting developers turn thinking on or off and set thinking budgets to balance quality, cost, and latency. Upgraded for rapid creative workflows, it can generate interleaved text and images and supports conversational, multi‑turn image editing in natural language. It’s also locale‑aware, enabling culturally and linguistically appropriate image generation for audiences worldwide.","secondaryModels":["gemini-2.5-flash-image-preview"],"contextSize":32768,"tags":["image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.5-flash-image","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash#image","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.3","outputCost":"2.5","isPreGateway":false},{"name":"Gemini 2.5 Flash Image Preview (Code name: Nano Banana)","chef":"google","primaryModel":"gemini-2.5-flash-image-preview","provider":"google","type":"chat","description":"Gemini 2.5 Flash Image Preview is our first fully hybrid reasoning model, letting developers turn thinking on or off and set thinking budgets to balance quality, cost, and latency. Upgraded for rapid creative workflows, it can generate interleaved text and images and supports conversational, multi‑turn image editing in natural language. It’s also locale‑aware, enabling culturally and linguistically appropriate image generation for audiences worldwide.","contextSize":32768,"tags":["image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash-image-preview","websiteUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-image-preview","modelUrl":"https://ai.google.dev/gemini-api/docs/models","pricingUrl":"https://ai.google.dev/pricing","inputCost":"0.3","outputCost":"2.5","isPreGateway":false},{"name":"Gemini 2.5 Flash Lite","chef":"google","primaryModel":"gemini-2.5-flash-lite","provider":"vertex","type":"chat","description":"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.","contextSize":1048576,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.5-flash-lite","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.1","outputCost":"0.4","cachedInputCost":"0.01","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash Lite","chef":"google","primaryModel":"gemini-2.5-flash-lite","provider":"google","type":"chat","description":"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.","contextSize":1048576,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash-lite","websiteUrl":"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite","pricingUrl":"https://ai.google.dev/gemini-api/docs/pricing","inputCost":"0.1","outputCost":"0.4","cachedInputCost":"0.01","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash Lite Preview 09-2025","chef":"google","primaryModel":"gemini-2.5-flash-lite-preview-09-2025","provider":"google","type":"chat","description":"Gemini 2.5 Flash-Lite is a balanced, low-latency model with configurable thinking budgets and tool connectivity (e.g., Google Search grounding and code execution). It supports multimodal input and offers a 1M-token context window.","contextSize":1048576,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash-lite-preview-09-2025","websiteUrl":"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite-preview","pricingUrl":"https://ai.google.dev/gemini-api/docs/pricing","inputCost":"0.1","outputCost":"0.4","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Flash Preview 09-2025","chef":"google","primaryModel":"gemini-2.5-flash-preview-09-2025","provider":"google","type":"chat","description":"Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.","contextSize":1000000,"tags":["file-input","implicit-caching","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-flash-preview-09-2025","websiteUrl":"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview","pricingUrl":"https://ai.google.dev/gemini-api/docs/pricing","inputCost":"0.3","outputCost":"2.5","cachedInputCost":"0.03","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Pro","chef":"google","primaryModel":"gemini-2.5-pro","provider":"vertex","type":"chat","description":"Gemini 2.5 Pro is our most advanced reasoning Gemini model, capable of solving complex problems. Gemini 2.5 Pro can comprehend vast datasets and challenging problems from different information sources, including text, audio, images, video, and even entire code repositories.","contextSize":1048576,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-2.5-pro","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini 2.5 Pro","chef":"google","primaryModel":"gemini-2.5-pro","provider":"google","type":"chat","description":"Gemini 2.5 Pro is our most advanced reasoning Gemini model, capable of solving complex problems. Gemini 2.5 Pro can comprehend vast datasets and challenging problems from different information sources, including text, audio, images, video, and even entire code repositories.","contextSize":1048576,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-2.5-pro","websiteUrl":"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/","modelUrl":"https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro","pricingUrl":"https://ai.google.dev/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","webSearchCallCost":"35.0","isPreGateway":false},{"name":"Gemini Embedding 001","chef":"google","primaryModel":"gemini-embedding-001","provider":"google","type":"embedding","description":"State-of-the-art embedding model with excellent performance across English, multilingual and code tasks.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/google:gemini-embedding-001","websiteUrl":"https://ai.google.dev/gemini-api/docs/embeddings","modelUrl":"https://ai.google.dev/gemini-api/docs/embeddings","pricingUrl":"https://ai.google.dev/gemini-api/docs/pricing#gemini-embedding","inputCost":"0.15","outputCost":"0","isPreGateway":false},{"name":"Gemini Embedding 001","chef":"google","primaryModel":"gemini-embedding-001","provider":"vertex","type":"embedding","description":"State-of-the-art embedding model with excellent performance across English, multilingual and code tasks.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/vertex:gemini-embedding-001","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.15","outputCost":"0","isPreGateway":false},{"name":"Text Embedding 005","chef":"google","primaryModel":"text-embedding-005","provider":"vertex","type":"embedding","description":"English-focused text embedding model optimized for code and English language tasks.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/vertex:text-embedding-005","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.025","outputCost":"0","isPreGateway":false},{"name":"Text Multilingual Embedding 002","chef":"google","primaryModel":"text-multilingual-embedding-002","provider":"vertex","type":"embedding","description":"Multilingual text embedding model optimized for cross-lingual tasks across many languages.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/vertex:text-multilingual-embedding-002","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.025","outputCost":"0","isPreGateway":false},{"name":"Mercury Coder Small Beta","chef":"inception","primaryModel":"mercury-coder-small","provider":"inception","type":"chat","description":"Mercury Coder Small is ideal for code generation, debugging, and refactoring tasks with minimal latency.","contextSize":32000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/inception:mercury-coder-small","websiteUrl":"https://platform.inceptionlabs.ai","modelUrl":"https://platform.inceptionlabs.ai/docs#models","pricingUrl":"https://platform.inceptionlabs.ai/docs#models","inputCost":"0.25","outputCost":"1.0","isPreGateway":false},{"name":"LongCat Flash Chat","chef":"meituan","primaryModel":"longcat-flash-chat","provider":"chutes","type":"chat","description":"LongCat-Flash-Chat is a high-throughput MoE chat model (128k context) designed for agentic tasks.","secondaryModels":["longcat-flash-chat-fp8"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/chutes:meituan-longcat/LongCat-Flash-Chat-FP8","websiteUrl":"https://chutes.ai","modelUrl":"https://chutes.ai/app/chute/2a6173bd-6d60-5ca6-8601-9ece77f055e4?tab=api","pricingUrl":"https://chutes.ai/app/chute/2a6173bd-6d60-5ca6-8601-9ece77f055e4?tab=api","inputCost":"0.0","outputCost":"0.0","isPreGateway":false},{"name":"LongCat Flash Chat","chef":"meituan","primaryModel":"longcat-flash-chat","provider":"meituan","type":"chat","description":"LongCat-Flash-Chat is a high-throughput MoE chat model (128k context) optimized for agentic tasks.","secondaryModels":["LongCat-Flash-Chat"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/meituan:LongCat-Flash-Chat","websiteUrl":"https://longcat.ai","modelUrl":"https://huggingface.co/meituan-longcat/LongCat-Flash-Chat","pricingUrl":"https://longcat.ai","inputCost":"0.0","outputCost":"0.0","isPreGateway":false},{"name":"LongCat Flash Thinking","chef":"meituan","primaryModel":"longcat-flash-thinking","provider":"chutes","type":"chat","description":"LongCat-Flash-Thinking is a high-throughput MoE reasoning model (128k context) optimized for agentic tasks.","secondaryModels":["longcat-flash-thinking-fp8"],"contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/chutes:meituan-longcat/LongCat-Flash-Thinking-FP8","websiteUrl":"https://chutes.ai","modelUrl":"https://chutes.ai/app/chute/5b3f1789-3247-5315-a7cd-78526725455e","pricingUrl":"https://chutes.ai/app/chute/5b3f1789-3247-5315-a7cd-78526725455e","inputCost":"0.15","outputCost":"1.5","isPreGateway":false},{"name":"Llama 3.1 70B Instruct","chef":"meta","primaryModel":"llama-3.1-70b","provider":"bedrock","type":"chat","description":"An update to Meta Llama 3 70B Instruct that includes an expanded 128K context length, multilinguality and improved reasoning capabilities.","secondaryModels":["llama3-1-70b-instruct-v1"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-1-70b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.72","outputCost":"0.72","isPreGateway":false},{"name":"Llama 3.1 8B","chef":"meta","primaryModel":"llama-3.1-8b","provider":"cerebras","type":"chat","description":"Llama 3.1 8B brings powerful performance in a smaller, more efficient package. With improved multilingual support, tool use, and a 128K context length, it enables sophisticated use cases like interactive agents and compact coding assistants while remaining lightweight and accessible.","secondaryModels":["llama3.1-8b"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cerebras:llama3.1-8b","websiteUrl":"https://inference-docs.cerebras.ai","modelUrl":"https://inference-docs.cerebras.ai/introduction","pricingUrl":"https://inference-docs.cerebras.ai/support/pricing","inputCost":"0.10","outputCost":"0.10","isPreGateway":false},{"name":"Llama 3.1 8B Instant","chef":"meta","primaryModel":"llama-3.1-8b","provider":"groq","type":"chat","description":"Llama 3.1 8B with 128K context window support, making it ideal for real-time conversational interfaces and data analysis while offering significant cost savings compared to larger models. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.","secondaryModels":["llama-3.1-8b-instant"],"contextSize":131000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:llama-3.1-8b-instant","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/playground?model=llama-3.1-8b-reasoning","pricingUrl":"https://wow.groq.com/","inputCost":"0.05","outputCost":"0.08","isPreGateway":false},{"name":"Llama 3.1 8B Instruct","chef":"meta","primaryModel":"llama-3.1-8b","provider":"bedrock","type":"chat","description":"An update to Meta Llama 3 8B Instruct that includes an expanded 128K context length, multilinguality and improved reasoning capabilities.","secondaryModels":["llama3-1-8b-instruct-v1"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-1-8b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.22","outputCost":"0.22","isPreGateway":false},{"name":"Llama 3.2 11B Vision Instruct","chef":"meta","primaryModel":"llama-3.2-11b","provider":"bedrock","type":"chat","description":"Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.","secondaryModels":["llama3-2-11b-instruct-v1"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-2-11b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.16","outputCost":"0.16","isPreGateway":false},{"name":"Llama 3.2 1B Instruct","chef":"meta","primaryModel":"llama-3.2-1b","provider":"bedrock","type":"chat","description":"Text-only model, supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.","secondaryModels":["llama3-2-1b-instruct-v1"],"contextSize":128000,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-2-1b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.1","outputCost":"0.1","isPreGateway":false},{"name":"Llama 3.2 3B Instruct","chef":"meta","primaryModel":"llama-3.2-3b","provider":"bedrock","type":"chat","description":"Text-only model, fine-tuned for supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.","secondaryModels":["llama3-2-3b-instruct-v1"],"contextSize":128000,"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-2-3b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.15","outputCost":"0.15","isPreGateway":false},{"name":"Llama 3.2 90B Vision Instruct","chef":"meta","primaryModel":"llama-3.2-90b","provider":"bedrock","type":"chat","description":"Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.","secondaryModels":["llama3-2-90b-instruct-v1"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-2-90b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.72","outputCost":"0.72","isPreGateway":false},{"name":"Llama 3.3 70B","chef":"meta","primaryModel":"llama-3.3-70b","provider":"cerebras","type":"chat","description":"The upgraded Llama 3.1 70B model features enhanced reasoning, tool use, and multilingual abilities, along with a significantly expanded 128K context window. These improvements make it well-suited for demanding tasks such as long-form summarization, multilingual conversations, and coding assistance.","contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cerebras:llama-3.3-70b","websiteUrl":"https://inference-docs.cerebras.ai","modelUrl":"https://inference-docs.cerebras.ai/introduction","pricingUrl":"https://inference-docs.cerebras.ai/support/pricing","inputCost":"0.85","outputCost":"1.20","isPreGateway":false},{"name":"Llama 3.3 70B Instruct","chef":"meta","primaryModel":"llama-3.3-70b","provider":"bedrock","type":"chat","description":"Where performance meets efficiency. This model supports high-performance conversational AI designed for content creation, enterprise applications, and research, offering advanced language understanding capabilities, including text summarization, classification, sentiment analysis, and code generation.","secondaryModels":["llama3-3-70b-instruct-v1"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama3-3-70b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.72","outputCost":"0.72","isPreGateway":false},{"name":"Llama 3.3 70B Versatile","chef":"meta","primaryModel":"llama-3.3-70b","provider":"groq","type":"chat","description":"The Meta Llama 3.3 multilingual model is a pretrained and instruction tuned generative model with 70B parameters. Optimized for multilingual dialogue use cases, it  outperforms many of the available open source and closed chat models on common industry benchmarks. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.","secondaryModels":["llama-3.3-70b-versatile"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:llama-3.3-70b-versatile","websiteUrl":"https://groq.com","modelUrl":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md","pricingUrl":"https://wow.groq.com/","inputCost":"0.59","outputCost":"0.79","isPreGateway":false},{"name":"Llama 4 Maverick 17B 128E Instruct","chef":"meta","primaryModel":"llama-4-maverick","provider":"vertex","type":"chat","description":"Llama 4 Maverick 17B-128E is Llama 4\'s largest and most capable model. It uses the Mixture-of-Experts (MoE) architecture and early fusion to provide coding, reasoning, and image capabilities.","secondaryModels":["llama-4-maverick-17b-128e-instruct-maas","llama-4-maverick-17b"],"contextSize":1310720,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:llama-4-maverick-17b-128e-instruct-maas","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-maverick","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.35","outputCost":"1.15","isPreGateway":false},{"name":"Llama 4 Maverick 17B 128E Instruct FP8","chef":"meta","primaryModel":"llama-4-maverick","provider":"deepinfra","type":"chat","description":"The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Maverick, a 17 billion parameter model with 128 experts. Served by DeepInfra.","secondaryModels":["llama-4-maverick-17b-128e-instruct-fp8","llama-4-maverick-17b"],"contextSize":131072,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"Llama 4 Maverick 17B Instruct","chef":"meta","primaryModel":"llama-4-maverick","provider":"bedrock","type":"chat","description":"As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B.","secondaryModels":["llama4-maverick-17b-instruct-v1","llama-4-maverick-17b"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama4-maverick-17b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.24","outputCost":"0.97","isPreGateway":false},{"name":"Llama 4 Scout 17B 16E Instruct","chef":"meta","primaryModel":"llama-4-scout","provider":"deepinfra","type":"chat","description":"The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Scout, a 17 billion parameter model with 16 experts. Served by DeepInfra.","secondaryModels":["llama-4-scout-17b-16e-instruct","llama-4-scout-17b"],"contextSize":131072,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:meta-llama/Llama-4-Scout-17B-16E-Instruct","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/meta-llama/Llama-4-Scout-17B-16E-Instruct","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.08","outputCost":"0.3","isPreGateway":false},{"name":"Llama 4 Scout 17B 16E Instruct","chef":"meta","primaryModel":"llama-4-scout","provider":"groq","type":"chat","description":"Llama 4 Scout is Meta\'s natively multimodal model with a 17B parameter mixture-of-experts architecture (16 experts), offering exceptional performance across text and image understanding with support for 12 languages, optimized for assistant-like chat, image recognition, and coding tasks. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.","secondaryModels":["llama-4-scout-17b-16e-instruct","llama-4-scout-17b"],"contextSize":131072,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:meta-llama/llama-4-scout-17b-16e-instruct","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/model/llama-4-scout-17b-16e-instruct","pricingUrl":"https://wow.groq.com/","inputCost":"0.11","outputCost":"0.34","isPreGateway":false},{"name":"Llama 4 Scout 17B 16E Instruct","chef":"meta","primaryModel":"llama-4-scout","provider":"vertex","type":"chat","description":"Llama 4 Scout 17B 16E Instruct is a multimodal model that uses the Mixture-of-Experts (MoE) architecture and early fusion, delivering state-of-the-art results for its size class.","secondaryModels":["llama-4-scout-17b-16e-instruct-maas","llama-4-scout-17b"],"contextSize":1310720,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vertex:llama-4-scout-17b-16e-instruct-maas","websiteUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/models","modelUrl":"https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout","pricingUrl":"https://cloud.google.com/vertex-ai/generative-ai/pricing","inputCost":"0.25","outputCost":"0.7","isPreGateway":false},{"name":"Llama 4 Scout 17B Instruct","chef":"meta","primaryModel":"llama-4-scout","provider":"bedrock","type":"chat","description":"Llama 4 Scout is the best multimodal model in the world in its class and is more powerful than our Llama 3 models, while fitting in a single H100 GPU. Additionally, Llama 4 Scout supports an industry-leading context window of up to 10M tokens.","secondaryModels":["llama4-scout-17b-instruct-v1","llama-4-scout-17b"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:us.meta.llama4-scout-17b-instruct-v1:0","websiteUrl":"https://aws.amazon.com/bedrock/meta/","modelUrl":"https://aws.amazon.com/bedrock/meta/","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.17","outputCost":"0.66","isPreGateway":false},{"name":"MiniMax M2","chef":"minimax","primaryModel":"minimax-m2","provider":"minimax","type":"chat","description":"MiniMax-M2 redefines efficiency for agents. It is a compact, fast, and cost-effective MoE model (230 billion total parameters with 10 billion active parameters) built for elite performance in coding and agentic tasks, all while maintaining powerful general intelligence.","secondaryModels":["minimax-m2-stable"],"contextSize":205000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/minimax:MiniMax-M2-Stable","websiteUrl":"https://www.minimax.io/","modelUrl":"https://www.minimax.io/news/minimax-m2","pricingUrl":"https://platform.minimax.io/docs/guides/pricing","inputCost":"0.3","outputCost":"1.2","cachedInputCost":"0.03","cacheCreationInputCost":"0.375","isPreGateway":false},{"name":"Codestral Embed","chef":"mistral","primaryModel":"codestral-embed","provider":"mistral","type":"embedding","description":"Code embedding model that can embed code databases and repositories to power coding assistants.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/mistral:codestral-embed","websiteUrl":"https://mistral.ai/","modelUrl":"https://docs.mistral.ai/capabilities/embeddings/code_embeddings/","pricingUrl":"https://mistral.ai/pricing#api-pricing","inputCost":"0.15","outputCost":"0","isPreGateway":false},{"name":"Devstral Small","chef":"mistral","primaryModel":"devstral-small","provider":"mistral","type":"chat","description":"Devstral is an agentic LLM for software engineering tasks built under a collaboration between Mistral AI and All Hands AI 🙌. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents.","secondaryModels":["devstral-small-2507"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:devstral-small-2507","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/devstral-2507","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.1","outputCost":"0.3","isPreGateway":false},{"name":"Magistral Medium 2506","chef":"mistral","primaryModel":"magistral-medium-2506","provider":"mistral","type":"chat","description":"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.","contextSize":128000,"tags":["reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:magistral-medium-2506","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/mistral-small-3-1/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"2.0","outputCost":"5.0","isPreGateway":false},{"name":"Magistral Medium 2509","chef":"mistral","primaryModel":"magistral-medium","provider":"mistral","type":"chat","description":"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.","secondaryModels":["magistral-medium-2509"],"contextSize":128000,"tags":["reasoning","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:magistral-medium-2509","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/mistral-small-3-1/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"2.0","outputCost":"5.0","isPreGateway":false},{"name":"Magistral Small 2506","chef":"mistral","primaryModel":"magistral-small-2506","provider":"mistral","type":"chat","description":"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.","contextSize":128000,"tags":["reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:magistral-small-2506","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/magistral","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.5","outputCost":"1.5","isPreGateway":false},{"name":"Magistral Small 2509","chef":"mistral","primaryModel":"magistral-small","provider":"mistral","type":"chat","description":"Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.","secondaryModels":["magistral-small-2509"],"contextSize":128000,"tags":["reasoning","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:magistral-small-2509","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/magistral","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.5","outputCost":"1.5","isPreGateway":false},{"name":"Ministral 3B","chef":"mistral","primaryModel":"ministral-3b","provider":"mistral","type":"chat","description":"A compact, efficient model for on-device tasks like smart assistants and local analytics, offering low-latency performance.","secondaryModels":["ministral-3b-latest"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:ministral-3b-latest","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/ministraux/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.04","outputCost":"0.04","isPreGateway":false},{"name":"Ministral 8B","chef":"mistral","primaryModel":"ministral-8b","provider":"mistral","type":"chat","description":"A more powerful model with faster, memory-efficient inference, ideal for complex workflows and demanding edge applications.","secondaryModels":["ministral-8b-latest"],"contextSize":128000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:ministral-8b-latest","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/ministraux/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.1","outputCost":"0.1","isPreGateway":false},{"name":"Mistral Codestral 25.01","chef":"mistral","primaryModel":"codestral","provider":"mistral","type":"chat","description":"Mistral Codestral 25.01 is a state-of-the-art coding model optimized for low-latency, high-frequency use cases. Proficient in over 80 programming languages, it excels at tasks like fill-in-the-middle (FIM), code correction, and test generation.","secondaryModels":["codestral-2501"],"contextSize":256000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:codestral-2501","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/technology/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.3","outputCost":"0.9","isPreGateway":false},{"name":"Mistral Embed","chef":"mistral","primaryModel":"mistral-embed","provider":"mistral","type":"embedding","description":"General-purpose text embedding model for semantic search, similarity, clustering, and RAG workflows.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/mistral:mistral-embed","websiteUrl":"https://mistral.ai/","modelUrl":"https://docs.mistral.ai/capabilities/embeddings/text_embeddings/","pricingUrl":"https://mistral.ai/pricing#api-pricing","inputCost":"0.1","outputCost":"0","isPreGateway":false},{"name":"Mistral Large","chef":"mistral","primaryModel":"mistral-large","provider":"mistral","type":"chat","description":"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents.","secondaryModels":["mistral-large-latest"],"contextSize":32000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:mistral-large-latest","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/mistral-large/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"2.0","outputCost":"6.0","isPreGateway":false},{"name":"Mistral Medium 3.1","chef":"mistral","primaryModel":"mistral-medium","provider":"mistral","type":"chat","description":"Mistral Medium 3 delivers frontier performance while being an order of magnitude less expensive. For instance, the model performs at or above 90% of Claude Sonnet 3.7 on benchmarks across the board at a significantly lower cost.","secondaryModels":["mistral-medium-2508"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:mistral-medium-2508","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/mistral-medium-3","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.4","outputCost":"2.0","isPreGateway":false},{"name":"Mistral Small","chef":"mistral","primaryModel":"mistral-small","provider":"mistral","type":"chat","description":"Mistral Small is the ideal choice for simple tasks that one can do in bulk - like Classification, Customer Support, or Text Generation. It offers excellent performance at an affordable price point.","secondaryModels":["mistral-small-latest"],"contextSize":32000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:mistral-small-latest","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/technology/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.1","outputCost":"0.3","isPreGateway":false},{"name":"Mixtral MoE 8x22B Instruct","chef":"mistral","primaryModel":"mixtral-8x22b-instruct","provider":"fireworks","type":"chat","description":"8x22b Instruct model. 8x22b is mixture-of-experts open source model by Mistral served by Fireworks.","contextSize":65536,"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/mixtral-8x22b-instruct","websiteUrl":"https://x.com/FireworksAI_HQ/status/1778617118583586852","modelUrl":"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct","pricingUrl":"https://fireworks.ai/models/fireworks/mixtral-8x22b-instruct","inputCost":"1.2","outputCost":"1.2","isPreGateway":false},{"name":"Pixtral 12B 2409","chef":"mistral","primaryModel":"pixtral-12b","provider":"mistral","type":"chat","description":"A 12B model with image understanding capabilities in addition to text.","secondaryModels":["pixtral-12b-2409"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:pixtral-12b-2409","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/pixtral-12b/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"0.15","outputCost":"0.15","isPreGateway":false},{"name":"Pixtral Large","chef":"mistral","primaryModel":"pixtral-large","provider":"mistral","type":"chat","description":"Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.","secondaryModels":["pixtral-large-latest"],"contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/mistral:pixtral-large-latest","websiteUrl":"https://mistral.ai/","modelUrl":"https://mistral.ai/news/pixtral-large/","pricingUrl":"https://docs.mistral.ai/platform/pricing/","inputCost":"2.0","outputCost":"6.0","isPreGateway":false},{"name":"Kimi K2","chef":"moonshotai","primaryModel":"kimi-k2","provider":"deepinfra","type":"chat","description":"Kimi K2 is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks.","secondaryModels":["kimi-k2-instruct"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/deepinfra:moonshotai/Kimi-K2-Instruct","websiteUrl":"https://deepinfra.com","modelUrl":"https://deepinfra.com/moonshotai/Kimi-K2-Instruct","pricingUrl":"https://deepinfra.com/pricing","inputCost":"0.5","outputCost":"2.0","isPreGateway":false},{"name":"Kimi K2","chef":"moonshotai","primaryModel":"kimi-k2","provider":"fireworks","type":"chat","description":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.","secondaryModels":["kimi-k2-instruct"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/kimi-k2-instruct","websiteUrl":"https://fireworks.ai","modelUrl":"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct","pricingUrl":"https://fireworks.ai/pricing","inputCost":"0.6","outputCost":"2.5","isPreGateway":false},{"name":"Kimi K2","chef":"moonshotai","primaryModel":"kimi-k2","provider":"groq","type":"chat","description":"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.","secondaryModels":["kimi-k2-instruct"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:moonshotai/kimi-k2-instruct","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct","pricingUrl":"https://wow.groq.com/","inputCost":"1.0","outputCost":"3.0","isPreGateway":false},{"name":"Kimi K2","chef":"moonshotai","primaryModel":"kimi-k2","provider":"moonshotai","type":"chat","description":"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.","secondaryModels":["kimi-k2-0711-preview"],"contextSize":131072,"tags":["implicit-caching","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/moonshotai:kimi-k2-0711-preview","websiteUrl":"https://www.moonshot.ai","modelUrl":"https://moonshotai.github.io/Kimi-K2/","pricingUrl":"https://platform.moonshot.ai/docs/pricing/chat","inputCost":"0.6","outputCost":"2.5","isPreGateway":false},{"name":"Kimi K2","chef":"moonshotai","primaryModel":"kimi-k2","provider":"parasail","type":"chat","description":"Kimi K2 is a model with a context length of 128k, featuring powerful code and Agent capabilities based on MoE architecture. It has 1T total parameters with 32B activated parameters. In benchmark performance tests across major categories including general knowledge reasoning, programming, mathematics, and Agent capabilities, the K2 model outperforms other mainstream open-source models.","secondaryModels":["kimi-k2-instruct"],"contextSize":130000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/parasail:parasail-kimi-k2-instruct","websiteUrl":"https://www.parasail.io/","modelUrl":"https://moonshotai.github.io/Kimi-K2/","pricingUrl":"https://www.saas.parasail.io/pricing","inputCost":"0.99","outputCost":"2.99","isPreGateway":false},{"name":"Kimi K2 0905","chef":"moonshotai","primaryModel":"kimi-k2-0905","provider":"baseten","type":"chat","description":"Kimi K2 0905 has shown strong performance on agentic tasks thanks to its tool calling, reasoning abilities, and long context handling. But as a large parameter model (1T parameters), it’s also resource-intensive. Running it in production requires a highly optimized inference stack to avoid excessive latency.","secondaryModels":["kimi-k2-instruct-0905"],"contextSize":131072,"playgroundUrl":"https://ai-sdk.dev/playground/baseten:moonshotai/Kimi-K2-Instruct-0905","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/kimi-k2-0905/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"0.60","outputCost":"2.50","isPreGateway":false},{"name":"Kimi K2 0905","chef":"moonshotai","primaryModel":"kimi-k2-0905","provider":"fireworks","type":"chat","description":"Kimi K2 0905 is an updated version of Kimi K2, a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Kimi K2 0905 has improved coding abilities, a longer context window, and agentic tool use, and a longer (262K) context window.","secondaryModels":["kimi-k2-instruct-0905"],"contextSize":256000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/kimi-k2-instruct-0905","websiteUrl":"https://fireworks.ai","modelUrl":"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905","pricingUrl":"https://app.fireworks.ai/models/fireworks/kimi-k2-instruct-0905","inputCost":"0.6","outputCost":"2.5","isPreGateway":false},{"name":"Kimi K2 0905","chef":"moonshotai","primaryModel":"kimi-k2-0905","provider":"groq","type":"chat","description":"Kimi K2 0905 is Moonshot AI\'s improved version of the Kimi K2 model, featuring enhanced coding capabilities with superior frontend development and tool calling performance. This Mixture-of-Experts (MoE) model with 1 trillion total parameters and 32 billion activated parameters offers improved integration with various agent scaffolds, making it ideal for building sophisticated AI agents and autonomous systems.","secondaryModels":["kimi-k2-instruct-0905"],"contextSize":262144,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:moonshotai/kimi-k2-instruct-0905","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct-0905","pricingUrl":"https://wow.groq.com/pricing","inputCost":"1.0","outputCost":"3.0","isPreGateway":false},{"name":"Kimi K2 0905","chef":"moonshotai","primaryModel":"kimi-k2-0905","provider":"moonshotai","type":"chat","description":"Kimi K2 is a model with a context length of 256k, featuring stronger Agentic Coding capabilities, more prominent front-end code aesthetics and practicality, and better context understanding capabilities based on the capabilities of kimi-k2-0711-preview.","secondaryModels":["kimi-k2-0905-preview"],"contextSize":256000,"tags":["implicit-caching","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/moonshotai:kimi-k2-0905-preview","websiteUrl":"https://www.moonshot.ai","modelUrl":"https://moonshotai.github.io/Kimi-K2/","pricingUrl":"https://platform.moonshot.ai/docs/pricing/chat","inputCost":"0.6","outputCost":"2.5","isPreGateway":false},{"name":"Kimi K2 Instruct","chef":"moonshotai","primaryModel":"kimi-k2","provider":"novita","type":"chat","description":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.","secondaryModels":["kimi-k2-instruct"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:moonshotai/kimi-k2-instruct","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.57","outputCost":"2.3","isPreGateway":false},{"name":"Kimi K2 Thinking","chef":"moonshotai","primaryModel":"kimi-k2-thinking","provider":"moonshotai","type":"chat","description":"Kimi K2 Thinking is an advanced open-source thinking model by Moonshot AI. It can execute up to 200 – 300 sequential tool calls without human interference, reasoning coherently across hundreds of steps to solve complex problems. Built as a thinking agent, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity\'s Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.","secondaryModels":[],"contextSize":262114,"tags":["reasoning","tool-use","implicit-caching"],"playgroundUrl":"https://ai-sdk.dev/playground/moonshotai:kimi-k2-thinking","websiteUrl":"https://www.moonshot.ai","modelUrl":"https://moonshotai.github.io/Kimi-K2/thinking.html","pricingUrl":"https://platform.moonshot.ai/docs/pricing/chat","inputCost":"0.60","outputCost":"2.50","cachedInputCost":"0.15","isPreGateway":false},{"name":"Kimi K2 Thinking","chef":"moonshotai","primaryModel":"kimi-k2-thinking","provider":"fireworks","type":"chat","description":"Kimi K2 Thinking is an advanced open-source thinking model by Moonshot AI. It can execute up to 200 – 300 sequential tool calls without human interference, reasoning coherently across hundreds of steps to solve complex problems. Built as a thinking agent, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity\'s Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.","secondaryModels":[],"contextSize":262114,"tags":["reasoning","tool-use","implicit-caching"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/kimi-k2-thinking","websiteUrl":"https://fireworks.ai/","modelUrl":"https://fireworks.ai/models/fireworks/kimi-k2-thinking","pricingUrl":"https://fireworks.ai/models/fireworks/kimi-k2-thinking","inputCost":"0.6","outputCost":"2.5","isPreGateway":false},{"name":"Kimi K2 Thinking Turbo","chef":"moonshotai","primaryModel":"kimi-k2-thinking-turbo","provider":"moonshotai","type":"chat","description":"High-speed version of kimi-k2-thinking, suitable for scenarios requiring both deep reasoning and extremely fast responses","secondaryModels":[],"contextSize":262114,"tags":["reasoning","tool-use","implicit-caching"],"playgroundUrl":"https://ai-sdk.dev/playground/moonshotai:kimi-k2-thinking-turbo","websiteUrl":"https://www.moonshot.ai","modelUrl":"https://moonshotai.github.io/Kimi-K2/thinking.html","pricingUrl":"https://platform.moonshot.ai/docs/pricing/chat","inputCost":"1.15","outputCost":"8","cachedInputCost":"0.15","isPreGateway":false},{"name":"Kimi K2 Turbo","chef":"moonshotai","primaryModel":"kimi-k2-turbo","provider":"moonshotai","type":"chat","description":"Kimi K2 Turbo is the high-speed version of kimi-k2, with the same model parameters as kimi-k2, but the output speed is increased to 60 tokens per second, with a maximum of 100 tokens per second, the context length is 256k","secondaryModels":["kimi-k2-turbo-preview"],"contextSize":256000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/moonshotai:kimi-k2-turbo-preview","websiteUrl":"https://www.moonshot.ai","modelUrl":"https://moonshotai.github.io/Kimi-K2/","pricingUrl":"https://platform.moonshot.ai/docs/pricing/chat","inputCost":"2.4","outputCost":"10.0","isPreGateway":false},{"name":"Morph V3 Fast","chef":"morph","primaryModel":"morph-v3-fast","provider":"morph","type":"chat","description":"Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 4500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.","contextSize":81920,"playgroundUrl":"https://ai-sdk.dev/playground/morph:morph-v3-fast","websiteUrl":"https://morphllm.com/","modelUrl":"https://morphllm.com/blog/what-is-morph-for","pricingUrl":"https://morphllm.com/dashboard","inputCost":"0.8","outputCost":"1.2","isPreGateway":false},{"name":"Morph V3 Large","chef":"morph","primaryModel":"morph-v3-large","provider":"morph","type":"chat","description":"Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 2500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.","contextSize":81920,"playgroundUrl":"https://ai-sdk.dev/playground/morph:morph-v3-large","websiteUrl":"https://morphllm.com/","modelUrl":"https://morphllm.com/blog/what-is-morph-for","pricingUrl":"https://morphllm.com/dashboard","inputCost":"0.9","outputCost":"1.9","isPreGateway":false},{"name":"GPT 5.1 Thinking","chef":"openai","primaryModel":"gpt-5.1-thinking","provider":"openai","type":"chat","description":"An upgraded version of GPT-5 that adapts thinking time more precisely to the question to spend more time on complex questions and respond more quickly to simpler tasks.","secondaryModels":["gpt-5.1"],"contextSize":400000,"tags":["tool-use","implicit-caching","file-input","image-generation","reasoning","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5.1","websiteUrl":"https://openai.com","modelUrl":"https://openai.com/index/gpt-5-1/","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","isPreGateway":false},{"name":"GPT-3.5 Turbo","chef":"openai","primaryModel":"gpt-3.5-turbo","provider":"openai","type":"chat","description":"OpenAI\'s most capable and cost effective model in the GPT-3.5 family optimized for chat purposes, but also works well for traditional completions tasks.","secondaryModels":["gpt-3.5-turbo-0125"],"contextSize":16385,"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-3.5-turbo","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-3-5","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.5","outputCost":"1.5","isPreGateway":false},{"name":"GPT-3.5 Turbo Instruct","chef":"openai","primaryModel":"gpt-3.5-turbo-instruct","provider":"openai","type":"chat","description":"Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.","contextSize":8192,"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-3.5-turbo-instruct","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-3-5","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.5","outputCost":"2.0","isPreGateway":false},{"name":"GPT-4 Turbo","chef":"openai","primaryModel":"gpt-4-turbo","provider":"openai","type":"chat","description":"gpt-4-turbo from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It has a knowledge cutoff of April 2023 and a 128,000 token context window.","contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4-turbo","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"10.0","outputCost":"30.0","isPreGateway":false},{"name":"GPT-4.1","chef":"openai","primaryModel":"gpt-4.1","provider":"azure","type":"chat","description":"GPT 4.1 is OpenAI\'s flagship model for complex tasks. It is well suited for problem solving across domains.","contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-4.1","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"2.0","outputCost":"8.0","cachedInputCost":"0.5","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4.1","chef":"openai","primaryModel":"gpt-4.1","provider":"openai","type":"chat","description":"GPT 4.1 is OpenAI\'s flagship model for complex tasks. It is well suited for problem solving across domains.","secondaryModels":["gpt-4.1-2025-04-14"],"contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4.1","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"2.0","outputCost":"8.0","cachedInputCost":"0.5","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4.1 mini","chef":"openai","primaryModel":"gpt-4.1-mini","provider":"azure","type":"chat","description":"GPT 4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.","contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-4.1-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1-mini","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"0.4","outputCost":"1.6","cachedInputCost":"0.1","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4.1 mini","chef":"openai","primaryModel":"gpt-4.1-mini","provider":"openai","type":"chat","description":"GPT 4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.","secondaryModels":["gpt-4.1-mini-2025-04-14"],"contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4.1-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1-mini","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.4","outputCost":"1.6","cachedInputCost":"0.1","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4.1 nano","chef":"openai","primaryModel":"gpt-4.1-nano","provider":"azure","type":"chat","description":"GPT-4.1 nano is the fastest, most cost-effective GPT 4.1 model.","contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-4.1-nano","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1-nano","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"0.1","outputCost":"0.4","cachedInputCost":"0.025","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4.1 nano","chef":"openai","primaryModel":"gpt-4.1-nano","provider":"openai","type":"chat","description":"GPT-4.1 nano is the fastest, most cost-effective GPT 4.1 model.","secondaryModels":["gpt-4.1-nano-2025-04-14"],"contextSize":1047576,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4.1-nano","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4.1-nano","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.1","outputCost":"0.4","cachedInputCost":"0.025","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4o","chef":"openai","primaryModel":"gpt-4o","provider":"azure","type":"chat","description":"GPT-4o from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It matches GPT-4 Turbo performance with a faster and cheaper API.","contextSize":128000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-4o","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4o","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"2.5","outputCost":"10.0","cachedInputCost":"1.25","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4o","chef":"openai","primaryModel":"gpt-4o","provider":"openai","type":"chat","description":"GPT-4o from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It matches GPT-4 Turbo performance with a faster and cheaper API.","secondaryModels":["gpt-4o-2024-08-06","gpt-4o-2024-11-20","gpt-4o-2024-05-13"],"contextSize":128000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4o","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4o","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"2.5","outputCost":"10.0","cachedInputCost":"1.25","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4o mini","chef":"openai","primaryModel":"gpt-4o-mini","provider":"azure","type":"chat","description":"GPT-4o mini from OpenAI is their most advanced and cost-efficient small model. It is multi-modal (accepting text or image inputs and outputting text) and has higher intelligence than gpt-3.5-turbo but is just as fast.","secondaryModels":["gpt-4o-mini-2024-07-18"],"contextSize":128000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-4o-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4o-mini","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"0.15","outputCost":"0.6","cachedInputCost":"0.075","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-4o mini","chef":"openai","primaryModel":"gpt-4o-mini","provider":"openai","type":"chat","description":"GPT-4o mini from OpenAI is their most advanced and cost-efficient small model. It is multi-modal (accepting text or image inputs and outputting text) and has higher intelligence than gpt-3.5-turbo but is just as fast.","secondaryModels":["gpt-4o-mini-2024-07-18"],"contextSize":128000,"tags":["file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-4o-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-4o-mini","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.15","outputCost":"0.6","cachedInputCost":"0.075","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5","chef":"openai","primaryModel":"gpt-5","provider":"azure","type":"chat","description":"GPT-5 is OpenAI\'s flagship language model that excels at complex reasoning, broad real-world knowledge, code-intensive, and multi-step agentic tasks.","secondaryModels":["gpt-5-2025-08-07"],"contextSize":400000,"tags":["file-input","reasoning","tool-use","vision","image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-5","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"1.25","outputCost":"10.0","cachedInputCost":"0.13","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5","chef":"openai","primaryModel":"gpt-5","provider":"openai","type":"chat","description":"GPT-5 is OpenAI\'s flagship language model that excels at complex reasoning, broad real-world knowledge, code-intensive, and multi-step agentic tasks.","secondaryModels":["gpt-5-2025-08-07"],"contextSize":400000,"tags":["file-input","implicit-caching","reasoning","tool-use","vision","image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-2025-08-07","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10.0","cachedInputCost":"0.125","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5 Chat","chef":"openai","primaryModel":"gpt-5-chat","provider":"openai","type":"chat","description":"GPT-5 Chat points to the GPT-5 snapshot currently used in ChatGPT.","secondaryModels":["gpt-5-chat-latest"],"contextSize":128000,"tags":["tool-use","implicit-caching","file-input","image-generation","vision","reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-chat-latest","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-5-chat-latest","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","isPreGateway":false},{"name":"GPT-5 mini","chef":"openai","primaryModel":"gpt-5-mini","provider":"azure","type":"chat","description":"GPT-5 mini is a cost optimized model that excels at reasoning/chat tasks. It offers an optimal balance between speed, cost, and capability.","secondaryModels":["gpt-5-mini-2025-08-07"],"contextSize":400000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-5-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"0.25","outputCost":"2.0","cachedInputCost":"0.03","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5 mini","chef":"openai","primaryModel":"gpt-5-mini","provider":"openai","type":"chat","description":"GPT-5 mini is a cost optimized model that excels at reasoning/chat tasks. It offers an optimal balance between speed, cost, and capability.","secondaryModels":["gpt-5-mini-2025-08-07"],"contextSize":400000,"tags":["file-input","implicit-caching","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-mini-2025-08-07","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.25","outputCost":"2.0","cachedInputCost":"0.025","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5 nano","chef":"openai","primaryModel":"gpt-5-nano","provider":"azure","type":"chat","description":"GPT-5 nano is a high throughput model that excels at simple instruction or classification tasks.","secondaryModels":["gpt-5-nano-2025-08-07"],"contextSize":400000,"tags":["file-input","reasoning","tool-use","vision","image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-5-nano","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"0.05","outputCost":"0.40","cachedInputCost":"0.01","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5 nano","chef":"openai","primaryModel":"gpt-5-nano","provider":"openai","type":"chat","description":"GPT-5 nano is a high throughput model that excels at simple instruction or classification tasks.","secondaryModels":["gpt-5-nano-2025-08-07"],"contextSize":400000,"tags":["file-input","implicit-caching","reasoning","tool-use","vision","image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-nano-2025-08-07","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.05","outputCost":"0.40","cachedInputCost":"0.005","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5 pro","chef":"openai","primaryModel":"gpt-5-pro","provider":"openai","type":"chat","description":"GPT-5 pro uses more compute to think harder and provide consistently better answers. Since GPT-5 pro is designed to tackle tough problems, some requests may take several minutes to finish.","secondaryModels":["gpt-5-pro-2025-10-06"],"contextSize":400000,"tags":["file-input","implicit-caching","reasoning","tool-use","vision","image-generation"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-pro","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-5-pro","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"15.0","outputCost":"120.0","isPreGateway":false},{"name":"GPT-5-Codex","chef":"openai","primaryModel":"gpt-5-codex","provider":"azure","type":"chat","description":"GPT-5-Codex is a version of GPT-5 optimized for agentic coding tasks in Codex or similar environments.","contextSize":400000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:gpt-5-codex","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"1.25","outputCost":"10.0","cachedInputCost":"0.13","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5-Codex","chef":"openai","primaryModel":"gpt-5-codex","provider":"openai","type":"chat","description":"GPT-5-Codex is a version of GPT-5 optimized for agentic coding tasks in Codex or similar environments.","contextSize":400000,"tags":["file-input","implicit-caching","reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5-codex","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-5-codex","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10.0","cachedInputCost":"0.125","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"GPT-5.1 Codex mini","chef":"openai","primaryModel":"gpt-5.1-codex-mini","provider":"openai","type":"chat","description":"GPT-5.1 Codex mini is a smaller, faster, and cheaper version of GPT-5.1 Codex.","secondaryModels":[],"contextSize":400000,"tags":["reasoning","file-input","vision","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5.1-codex-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-5.1-codex-mini","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.25","outputCost":"2","cachedInputCost":"0.025","isPreGateway":false},{"name":"GPT-5.1 Instant","chef":"openai","primaryModel":"gpt-5.1-instant","provider":"openai","type":"chat","description":"GPT-5.1 Instant (or GPT-5.1 chat) is a warmer and more conversational version of GPT-5-chat, with improved instruction following and adaptive reasoning for deciding when to think before responding.","secondaryModels":["gpt-5.1-chat-latest","gpt-5.1-2025-11-13"],"contextSize":128000,"tags":["tool-use","vision","file-input","image-generation","reasoning","implicit-caching"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5.1-chat-latest","websiteUrl":"https://openai.com","modelUrl":"https://openai.com/index/gpt-5-1/","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","isPreGateway":false},{"name":"GPT-5.1-Codex","chef":"openai","primaryModel":"gpt-5.1-codex","provider":"openai","type":"chat","description":"GPT-5.1-Codex is a version of GPT-5.1 optimized for agentic coding tasks in Codex or similar environments.","secondaryModels":[],"contextSize":400000,"tags":["file-input","tool-use","reasoning","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:gpt-5.1-codex","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/gpt-5.1-codex","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.25","outputCost":"10","cachedInputCost":"0.125","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"baseten","type":"chat","description":"Extremely capable general-purpose LLM with strong, controllable reasoning capabilities","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/baseten:openai/gpt-oss-120b","websiteUrl":"https://www.baseten.co","modelUrl":"https://www.baseten.co/library/gpt-oss-120b/","pricingUrl":"https://www.baseten.co/pricing/","inputCost":"0.1","outputCost":"0.5","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"bedrock","type":"chat","description":"A high-performance, open-weight language model designed for production-grade, general-purpose use cases.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:openai.gpt-oss-120b-1:0","websiteUrl":"https://aws.amazon.com/bedrock/openai/","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"cerebras","type":"chat","description":"This model excels at efficient reasoning across science, math, and coding applications. It’s ideal for real-time coding assistance, processing large documents for Q&A and summarization, agentic research workflows, and regulated on-premises workloads.","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cerebras:gpt-oss-120b","websiteUrl":"https://inference-docs.cerebras.ai","modelUrl":"https://inference-docs.cerebras.ai/models/openai-oss","pricingUrl":"https://inference-docs.cerebras.ai/support/pricing","inputCost":"0.25","outputCost":"0.69","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"fireworks","type":"chat","description":"A high-performance, open-weight language model designed for production-grade, general-purpose use cases.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/gpt-oss-120b","websiteUrl":"https://fireworks.ai","modelUrl":"https://fireworks.ai/models/fireworks/gpt-oss-120b","pricingUrl":"https://fireworks.ai/models/fireworks/gpt-oss-120b","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"groq","type":"chat","description":"For production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters).","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:openai/gpt-oss-120b","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/models","pricingUrl":"https://wow.groq.com/","inputCost":"0.15","outputCost":"0.6","isPreGateway":false},{"name":"gpt-oss-120b","chef":"openai","primaryModel":"gpt-oss-120b","provider":"parasail","type":"chat","description":"This model excels at efficient reasoning across science, math, and coding applications. It’s ideal for real-time coding assistance, processing large documents for Q&A and summarization, agentic research workflows, and regulated on-premises workloads.","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/parasail:parasail-gpt-oss-120b","websiteUrl":"https://www.parasail.io/","modelUrl":"https://www.parasail.io/","pricingUrl":"https://www.saas.parasail.io/pricing","inputCost":"0.15","outputCost":"0.60","isPreGateway":false},{"name":"gpt-oss-20b","chef":"openai","primaryModel":"gpt-oss-20b","provider":"bedrock","type":"chat","description":"A compact, open-weight language model optimized for low-latency and resource-constrained environments, including local and edge deployments","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/bedrock:openai.gpt-oss-20b-1:0","websiteUrl":"https://aws.amazon.com/bedrock/openai/","modelUrl":"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-openai.html","pricingUrl":"https://aws.amazon.com/bedrock/pricing/","inputCost":"0.07","outputCost":"0.3","isPreGateway":false},{"name":"gpt-oss-20b","chef":"openai","primaryModel":"gpt-oss-20b","provider":"fireworks","type":"chat","description":"A compact, open-weight language model optimized for low-latency and resource-constrained environments, including local and edge deployments","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/fireworks:accounts/fireworks/models/gpt-oss-20b","websiteUrl":"https://fireworks.ai","modelUrl":"https://fireworks.ai/models/fireworks/gpt-oss-20b","pricingUrl":"https://fireworks.ai/models/fireworks/gpt-oss-20b","inputCost":"0.07","outputCost":"0.3","isPreGateway":false},{"name":"gpt-oss-20b","chef":"openai","primaryModel":"gpt-oss-20b","provider":"groq","type":"chat","description":"For lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters).","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:openai/gpt-oss-20b","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/models","pricingUrl":"https://wow.groq.com/","inputCost":"0.075","outputCost":"0.3","isPreGateway":false},{"name":"gpt-oss-safeguard-20b","chef":"openai","primaryModel":"gpt-oss-safeguard-20b","provider":"groq","type":"chat","description":"OpenAI\'s first open weight reasoning model specifically trained for safety classification tasks. Fine-tuned from GPT-OSS, this model helps classify text content based on customizable policies, enabling bring-your-own-policy Trust & Safety AI where your own taxonomy, definitions, and thresholds guide classification decisions.","secondaryModels":[],"contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/groq:openai/gpt-oss-safeguard-20b","websiteUrl":"https://groq.com","modelUrl":"https://console.groq.com/docs/model/openai/gpt-oss-safeguard-20b","pricingUrl":"https://groq.com/pricing","inputCost":"0.075","outputCost":"0.30","cachedInputCost":"0.037","isPreGateway":false},{"name":"o1","chef":"openai","primaryModel":"o1","provider":"azure","type":"chat","description":"o1 is OpenAI\'s flagship reasoning model, designed for complex problems that require deep thinking. It provides strong reasoning capabilities with improved accuracy for complex multi-step tasks.","contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:o1","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o1","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"15.0","outputCost":"60.0","cachedInputCost":"7.5","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o1","chef":"openai","primaryModel":"o1","provider":"openai","type":"chat","description":"o1 is OpenAI\'s flagship reasoning model, designed for complex problems that require deep thinking. It provides strong reasoning capabilities with improved accuracy for complex multi-step tasks.","secondaryModels":["o1-2024-12-17"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:o1","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o1","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"15.0","outputCost":"60.0","cachedInputCost":"7.5","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o3","chef":"openai","primaryModel":"o3","provider":"openai","type":"chat","description":"OpenAI\'s o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.","secondaryModels":["o3-2025-04-16"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:o3","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o3","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"2.0","outputCost":"8.0","cachedInputCost":"0.5","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o3-deep-research","chef":"openai","primaryModel":"o3-deep-research","provider":"openai","type":"chat","description":"o3-deep-research is OpenAI\'s most advanced model for deep research, designed to tackle complex, multi-step research tasks. It can search and synthesize information from across the internet as well as from your own data—brought in through MCP connectors.","secondaryModels":["o3-deep-research-2025-06-26"],"contextSize":200000,"tags":["reasoning","file-input","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:o3-deep-research","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o3-deep-research","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"10.00","outputCost":"40","cachedInputCost":"2.5","isPreGateway":false},{"name":"o3-mini","chef":"openai","primaryModel":"o3-mini","provider":"azure","type":"chat","description":"o3-mini is OpenAI\'s most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.","contextSize":200000,"tags":["file-input","reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:o3-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o3-mini","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"1.1","outputCost":"4.4","cachedInputCost":"0.55","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o3-mini","chef":"openai","primaryModel":"o3-mini","provider":"openai","type":"chat","description":"o3-mini is OpenAI\'s most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.","secondaryModels":["o3-mini-2025-01-31"],"contextSize":200000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:o3-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o3-mini","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.1","outputCost":"4.4","cachedInputCost":"0.55","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o4-mini","chef":"openai","primaryModel":"o4-mini","provider":"azure","type":"chat","description":"OpenAI\'s o4-mini delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math (best-performing on AIME benchmarks), coding, and visual tasks.","contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/azure:o4-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o4-mini","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service","inputCost":"1.1","outputCost":"4.4","cachedInputCost":"0.275","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"o4-mini","chef":"openai","primaryModel":"o4-mini","provider":"openai","type":"chat","description":"OpenAI\'s o4-mini delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math (best-performing on AIME benchmarks), coding, and visual tasks.","secondaryModels":["o4-mini-2025-04-16"],"contextSize":200000,"tags":["file-input","reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/openai:o4-mini","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/o4-mini","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"1.1","outputCost":"4.4","cachedInputCost":"0.275","cacheCreationInputCost":"0.0","isPreGateway":false},{"name":"text-embedding-3-large","chef":"openai","primaryModel":"text-embedding-3-large","provider":"azure","type":"embedding","description":"OpenAI\'s most capable embedding model for both english and non-english tasks.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/azure:text-embedding-3-large","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-3-large","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/","inputCost":"0.13","outputCost":"0","isPreGateway":false},{"name":"text-embedding-3-large","chef":"openai","primaryModel":"text-embedding-3-large","provider":"openai","type":"embedding","description":"OpenAI\'s most capable embedding model for both english and non-english tasks.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/openai:text-embedding-3-large","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-3-large","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.13","outputCost":"0","isPreGateway":false},{"name":"text-embedding-3-small","chef":"openai","primaryModel":"text-embedding-3-small","provider":"azure","type":"embedding","description":"OpenAI\'s improved, more performant version of their ada embedding model.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/azure:text-embedding-3-small","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-3-small","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/","inputCost":"0.02","outputCost":"0","isPreGateway":false},{"name":"text-embedding-3-small","chef":"openai","primaryModel":"text-embedding-3-small","provider":"openai","type":"embedding","description":"OpenAI\'s improved, more performant version of their ada embedding model.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/openai:text-embedding-3-small","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-3-small","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.02","outputCost":"0","isPreGateway":false},{"name":"text-embedding-ada-002","chef":"openai","primaryModel":"text-embedding-ada-002","provider":"azure","type":"embedding","description":"OpenAI\'s legacy text embedding model.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/azure:text-embedding-ada-002","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-ada-002","pricingUrl":"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/","inputCost":"0.10","outputCost":"0","isPreGateway":false},{"name":"text-embedding-ada-002","chef":"openai","primaryModel":"text-embedding-ada-002","provider":"openai","type":"embedding","description":"OpenAI\'s legacy text embedding model.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/openai:text-embedding-ada-002","websiteUrl":"https://openai.com","modelUrl":"https://platform.openai.com/docs/models/text-embedding-ada-002","pricingUrl":"https://platform.openai.com/docs/pricing","inputCost":"0.10","outputCost":"0","isPreGateway":false},{"name":"Sonar","chef":"perplexity","primaryModel":"sonar","provider":"perplexity","type":"chat","description":"Perplexity\'s lightweight offering with search grounding, quicker and cheaper than Sonar Pro.","contextSize":127000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/perplexity:sonar","websiteUrl":"https://perplexity.ai","modelUrl":"https://sonar.perplexity.ai","pricingUrl":"https://docs.perplexity.ai/docs/pricing","inputCost":"1.0","outputCost":"1.0","isPreGateway":false},{"name":"Sonar Pro","chef":"perplexity","primaryModel":"sonar-pro","provider":"perplexity","type":"chat","description":"Perplexity\'s premier offering with search grounding, supporting advanced queries and follow-ups.","contextSize":200000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/perplexity:sonar-pro","websiteUrl":"https://perplexity.ai","modelUrl":"https://sonar.perplexity.ai","pricingUrl":"https://docs.perplexity.ai/docs/pricing","inputCost":"3.0","outputCost":"15.0","isPreGateway":false},{"name":"Sonar Reasoning","chef":"perplexity","primaryModel":"sonar-reasoning","provider":"perplexity","type":"chat","description":"A reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing detailed explanations with search grounding.","contextSize":127000,"tags":["reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/perplexity:sonar-reasoning","websiteUrl":"https://perplexity.ai","modelUrl":"https://sonar.perplexity.ai","pricingUrl":"https://docs.perplexity.ai/guides/pricing","inputCost":"1.0","outputCost":"5.0","isPreGateway":false},{"name":"Sonar Reasoning Pro","chef":"perplexity","primaryModel":"sonar-reasoning-pro","provider":"perplexity","type":"chat","description":"A premium reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing comprehensive explanations with enhanced search capabilities and multiple search queries per request.","contextSize":127000,"tags":["reasoning"],"playgroundUrl":"https://ai-sdk.dev/playground/perplexity:sonar-reasoning-pro","websiteUrl":"https://perplexity.ai","modelUrl":"https://sonar.perplexity.ai","pricingUrl":"https://docs.perplexity.ai/guides/pricing","inputCost":"2.0","outputCost":"8.0","isPreGateway":false},{"name":"Sonoma Dusk Alpha","chef":"stealth","primaryModel":"sonoma-dusk-alpha","provider":"stealth","type":"chat","description":"This model is no longer in stealth and gets responses from Grok 4 Fast Non-Reasoning–xAI\'s latest multimodal model with SOTA cost-efficiency and a 2M token context window.","contextSize":2000000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/stealth:grok-4-fast-non-reasoning","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/","pricingUrl":"https://docs.x.ai/docs/models","inputCost":"0.2","outputCost":"0.5","cachedInputCost":"0.05","isPreGateway":false},{"name":"Sonoma Sky Alpha","chef":"stealth","primaryModel":"sonoma-sky-alpha","provider":"stealth","type":"chat","description":"This model is no longer in stealth and gets responses from Grok 4 Fast Reasoning–xAI\'s latest multimodal model with SOTA cost-efficiency and a 2M token context window.","contextSize":2000000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/stealth:grok-4-fast-reasoning","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/","pricingUrl":"https://docs.x.ai/docs/models","inputCost":"0.2","outputCost":"0.5","cachedInputCost":"0.05","isPreGateway":false},{"name":"v0-1.0-md","chef":"vercel","primaryModel":"v0-1.0-md","provider":"vercel","type":"chat","description":"Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.","contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vercel:v0-1.0-md","websiteUrl":"https://v0.app","modelUrl":"https://vercel.com/docs/v0/model-api","pricingUrl":"https://v0.app/pricing","inputCost":"3.00","outputCost":"15.00","isPreGateway":false},{"name":"v0-1.5-md","chef":"vercel","primaryModel":"v0-1.5-md","provider":"vercel","type":"chat","description":"Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.","contextSize":128000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/vercel:v0-1.5-md","websiteUrl":"https://v0.app/","modelUrl":"https://vercel.com/docs/v0/model-api","pricingUrl":"https://v0.app/pricing","inputCost":"3.0","outputCost":"15.0","isPreGateway":false},{"name":"voyage-3-large","chef":"voyage","primaryModel":"voyage-3-large","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model with the best general-purpose and multilingual retrieval quality.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-3-large","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.18","outputCost":"0","isPreGateway":false},{"name":"voyage-3.5","chef":"voyage","primaryModel":"voyage-3.5","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for general-purpose and multilingual retrieval quality.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-3.5","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.06","outputCost":"0","isPreGateway":false},{"name":"voyage-3.5-lite","chef":"voyage","primaryModel":"voyage-3.5-lite","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for latency and cost.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-3.5-lite","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.02","outputCost":"0","isPreGateway":false},{"name":"voyage-code-2","chef":"voyage","primaryModel":"voyage-code-2","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for code retrieval (17% better than alternatives). This is the previous generation of code embeddings models.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-code-2","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.12","outputCost":"0","isPreGateway":false},{"name":"voyage-code-3","chef":"voyage","primaryModel":"voyage-code-3","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for code retrieval.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-code-3","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.18","outputCost":"0","isPreGateway":false},{"name":"voyage-finance-2","chef":"voyage","primaryModel":"voyage-finance-2","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for finance retrieval and RAG.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-finance-2","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.12","outputCost":"0","isPreGateway":false},{"name":"voyage-law-2","chef":"voyage","primaryModel":"voyage-law-2","provider":"voyage","type":"embedding","description":"Voyage AI\'s embedding model optimized for legal retrieval and RAG.","contextSize":0,"playgroundUrl":"https://ai-sdk.dev/playground/voyage:voyage-law-2","websiteUrl":"https://voyageai.com","modelUrl":"https://docs.voyageai.com/docs/embeddings","pricingUrl":"https://docs.voyageai.com/docs/pricing","inputCost":"0.12","outputCost":"0","isPreGateway":false},{"name":"Grok 2","chef":"xai","primaryModel":"grok-2","provider":"xai","type":"chat","description":"Grok 2 is a frontier language model with state-of-the-art reasoning capabilities. It features advanced capabilities in chat, coding, and reasoning, outperforming both Claude 3.5 Sonnet and GPT-4-Turbo on the LMSYS leaderboard.","secondaryModels":["grok-2-1212"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-2-1212","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/blog/grok-2","pricingUrl":"https://console.x.ai","inputCost":"2.0","outputCost":"10.0","isPreGateway":false},{"name":"Grok 2 Vision","chef":"xai","primaryModel":"grok-2-vision","provider":"xai","type":"chat","description":"Grok 2 vision model excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and document-based question answering (DocVQA). It can process a wide variety of visual information including documents, diagrams, charts, screenshots, and photographs.","secondaryModels":["grok-2-vision-1212"],"contextSize":32768,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-2-vision-1212","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/blog/grok-2","pricingUrl":"https://console.x.ai","inputCost":"2.0","outputCost":"10.0","isPreGateway":false},{"name":"Grok 3 Beta","chef":"xai","primaryModel":"grok-3","provider":"xai","type":"chat","description":"xAI\'s flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.","secondaryModels":["grok-3-beta"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-3-beta","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/grok-3","pricingUrl":"https://console.x.ai","inputCost":"3.0","outputCost":"15.0","isPreGateway":false},{"name":"Grok 3 Fast Beta","chef":"xai","primaryModel":"grok-3-fast","provider":"xai","type":"chat","description":"xAI\'s flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.","secondaryModels":["grok-3-fast-beta"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-3-fast-beta","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/grok-3","pricingUrl":"https://console.x.ai","inputCost":"5.0","outputCost":"25.0","isPreGateway":false},{"name":"Grok 3 Mini Beta","chef":"xai","primaryModel":"grok-3-mini","provider":"xai","type":"chat","description":"xAI\'s lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.","secondaryModels":["grok-3-mini-beta"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-3-mini-beta","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/grok-3","pricingUrl":"https://console.x.ai","inputCost":"0.3","outputCost":"0.5","isPreGateway":false},{"name":"Grok 3 Mini Fast Beta","chef":"xai","primaryModel":"grok-3-mini-fast","provider":"xai","type":"chat","description":"xAI\'s lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.","secondaryModels":["grok-3-mini-fast-beta"],"contextSize":131072,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-3-mini-fast-beta","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/grok-3","pricingUrl":"https://console.x.ai","inputCost":"0.6","outputCost":"4.0","isPreGateway":false},{"name":"Grok 4","chef":"xai","primaryModel":"grok-4","provider":"xai","type":"chat","description":"xAI\'s latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.","secondaryModels":["grok-4-0709","grok-4-latest"],"contextSize":256000,"tags":["reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-4","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/","pricingUrl":"https://console.x.ai","inputCost":"3.0","outputCost":"15.0","isPreGateway":false},{"name":"Grok 4 Fast Non-Reasoning","chef":"xai","primaryModel":"grok-4-fast-non-reasoning","provider":"xai","type":"chat","description":"Grok 4 Fast is xAI\'s latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning.","contextSize":2000000,"tags":["tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-4-fast-non-reasoning","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/","pricingUrl":"https://docs.x.ai/docs/models","inputCost":"0.2","outputCost":"0.5","cachedInputCost":"0.05","isPreGateway":false},{"name":"Grok 4 Fast Reasoning","chef":"xai","primaryModel":"grok-4-fast-reasoning","provider":"xai","type":"chat","description":"Grok 4 Fast is xAI\'s latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning.","contextSize":2000000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-4-fast-reasoning","websiteUrl":"https://x.ai","modelUrl":"https://x.ai/news/","pricingUrl":"https://docs.x.ai/docs/models","inputCost":"0.2","outputCost":"0.5","cachedInputCost":"0.05","isPreGateway":false},{"name":"Grok Code Fast 1","chef":"xai","primaryModel":"grok-code-fast-1","provider":"xai","type":"chat","description":"xAI\'s latest coding model that offers fast agentic coding with a 256K context window.","secondaryModels":["grok-code-fast","grok-code-fast-1-0825"],"contextSize":256000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/xai:grok-code-fast-1","websiteUrl":"https://x.ai","modelUrl":"https://docs.x.ai/docs/models/grok-code-fast-1","pricingUrl":"https://docs.x.ai/docs/models/grok-code-fast-1","inputCost":"0.2","outputCost":"1.5","isPreGateway":false},{"name":"GLM 4.5","chef":"zai","primaryModel":"glm-4.5","provider":"zai","type":"chat","description":"GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/zai:glm-4.5","websiteUrl":"https://z.ai","modelUrl":"https://z.ai/blog/glm-4.5","pricingUrl":"https://docs.z.ai/guides/overview/pricing","inputCost":"0.6","outputCost":"2.2","isPreGateway":false},{"name":"GLM 4.5 Air","chef":"zai","primaryModel":"glm-4.5-air","provider":"zai","type":"chat","description":"GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.","contextSize":128000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/zai:glm-4.5-air","websiteUrl":"https://z.ai","modelUrl":"https://z.ai/blog/glm-4.5","pricingUrl":"https://docs.z.ai/guides/overview/pricing","inputCost":"0.2","outputCost":"1.1","isPreGateway":false},{"name":"GLM 4.5V","chef":"zai","primaryModel":"glm-4.5v","provider":"novita","type":"chat","description":"Built on the GLM-4.5-Air base model, GLM-4.5V inherits proven techniques from GLM-4.1V-Thinking while achieving effective scaling through a powerful 106B-parameter MoE architecture.","contextSize":65536,"tags":["reasoning","tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:zai-org/glm-4.5v","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.6","outputCost":"1.8","isPreGateway":false},{"name":"GLM 4.5V","chef":"zai","primaryModel":"glm-4.5v","provider":"zai","type":"chat","description":"Built on the GLM-4.5-Air base model, GLM-4.5V inherits proven techniques from GLM-4.1V-Thinking while achieving effective scaling through a powerful 106B-parameter MoE architecture.","contextSize":66000,"tags":["tool-use","vision"],"playgroundUrl":"https://ai-sdk.dev/playground/zai:glm-4.5v","websiteUrl":"https://z.ai","modelUrl":"https://docs.z.ai/guides/vlm/glm-4.5v","pricingUrl":"https://docs.z.ai/guides/overview/pricing","inputCost":"0.6","outputCost":"1.8","isPreGateway":false},{"name":"GLM 4.6","chef":"zai","primaryModel":"glm-4.6","provider":"zai","type":"chat","description":"As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications.","contextSize":200000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/zai:glm-4.6","websiteUrl":"https://z.ai","modelUrl":"https://z.ai/blog/glm-4.6","pricingUrl":"https://docs.z.ai/guides/overview/pricing","inputCost":"0.45","outputCost":"1.8","cachedInputCost":"0.11","isPreGateway":false},{"name":"GLM 4.6","chef":"zai","primaryModel":"glm-4.6","provider":"cerebras","type":"chat","description":"As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications.","secondaryModels":[],"contextSize":131000,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/cerebras:zai-glm-4.6","websiteUrl":"https://cerebras.ai","modelUrl":"https://inference-docs.cerebras.ai/models/zai-glm-46","pricingUrl":"https://www.cerebras.ai/pricing","inputCost":"2.25","outputCost":"2.75","isPreGateway":false},{"name":"GLM-4.5","chef":"zai","primaryModel":"glm-4.5","provider":"novita","type":"chat","description":"GLM-4.5 Series Models are foundation models specifically engineered for intelligent agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application demands. As a hybrid reasoning system, it offers dual operational modes.","contextSize":131072,"tags":["reasoning","tool-use"],"playgroundUrl":"https://ai-sdk.dev/playground/novita:zai-org/glm-4.5","websiteUrl":"https://novita.ai","modelUrl":"https://novita.ai/models","pricingUrl":"https://novita.ai/pricing","inputCost":"0.6","outputCost":"2.2","isPreGateway":false}]'))}]);

//# debugId=3977ce1d-d589-9929-0839-1aac398f3902
//# sourceMappingURL=37267a230ca8701a.js.map